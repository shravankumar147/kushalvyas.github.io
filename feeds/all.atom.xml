<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>BitsMakeMeCrazy Kushal Vyas's Blog</title><link href="http://kushalvyas.github.io/" rel="alternate"></link><link href="http://kushalvyas.github.io/feeds/all.atom.xml" rel="self"></link><id>http://kushalvyas.github.io/</id><updated>2017-01-20T00:40:00+05:30</updated><entry><title>Caffe + ConvNets : Visual Recognition MadeÂ Easy</title><link href="http://kushalvyas.github.io/caffe_cnn.html" rel="alternate"></link><updated>2017-01-20T00:40:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2017-01-20:caffe_cnn.html</id><summary type="html">&lt;p&gt;To classify, recognize and localize objects in an image is a hot topic in Computer Vision and  throughout the years various models have been established for the same. My previous post on &lt;a href="http://kushalvyas.github.io/BOV.html" target="_blank"&gt;Bag of Visual Words Model for Image Classification and Recognition&lt;/a&gt; illustrates one such model. Today I write about &lt;a href="http://cs231n.github.io/convolutional-networks/" target="_blank"&gt;Convolutional Neural Networks&lt;/a&gt; and how to implement them in &lt;a href="http://caffe.berkeleyvision.org" target="_blank"&gt;Caffe&lt;/a&gt;. and how to train a &lt;span class="caps"&gt;CNN&lt;/span&gt; for your own dataset. We will be using the standard dataset, but you can organize any other/personal dataset in a similar&amp;nbsp;fashion.&lt;/p&gt;
&lt;h2&gt;Basics of &lt;span class="caps"&gt;CNN&lt;/span&gt;&amp;nbsp;:&lt;/h2&gt;
&lt;p&gt;A typical Convolutional Neural Net or &lt;strong&gt;&lt;span class="caps"&gt;CNN&lt;/span&gt;&lt;/strong&gt;, is a feed-forward neural net, with the input being an image. Its&amp;#8217; major objective it to establish a hierarchy of spatial features present in an image using which it is able to classify. The architecture of the net comprises of convolutional layers, pooling layers , activation layer, and fully connected&amp;nbsp;layers.&lt;/p&gt;
&lt;p&gt;One can refer &lt;a href="http://cs231n.github.io/convolutional-networks/" target="_blank"&gt;this post by Andrej Karpathy&lt;/a&gt; to understand the intricate details of a&amp;nbsp;ConvNet.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="typical_conv" src="http://kushalvyas.github.io/images/cnn_images/typical_cnn.png" /&gt;&lt;/center&gt;
&lt;center&gt;Source : &lt;a href="https://en. wikipedia.org/wiki/File:Typical_cnn.png" target="_blank"&gt;Convolutional Neural Nets : Wiki&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Convolution Layer : &lt;span class="caps"&gt;CONV&lt;/span&gt;&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;The primary operator is a convolution operation. For a 1-D signal it is expressed&amp;nbsp;as 
&lt;/p&gt;
&lt;div class="math"&gt;$$ y(n) = x(n) * h(n) = \sum_{k = - \infty}^{k = + \infty}x[k] . h[n-k]$$&lt;/div&gt;
&lt;p&gt;However, an image is a 2D signal (stored as a 2D matrix). To convolve an image, we use a convolution kernel, which is simply a 2D&amp;nbsp;matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$ convolve(I_1, I_2) = I_1[x, y] * I_2[x, y] = \sum_{n_1= - \infty}^{ n_1 = + \infty} \text{   } \sum_{n_2 = - \infty}^{ n_2 = + \infty} I_1[n_1, n_2].I_2[x - n_1, y - n_2]$$&lt;/div&gt;
&lt;p&gt;Here&amp;#8217;s an example of how it&amp;nbsp;works&lt;/p&gt;
&lt;p&gt;&lt;img alt="cn2d" src="http://kushalvyas.github.io/images/cnn_images/con2d.png" /&gt;&lt;/p&gt;
&lt;p&gt;For this 7 x 7 matrix, and a kernel of 3 x 3, the kernel will slide over the matrix one column, at a time. Once it reaches the end of the matrix (columnwise), it&amp;#8217;ll shift to the next row. At every position, the dot product is&amp;nbsp;computed. &lt;/p&gt;
&lt;p&gt;If the kernel is overlapped with the (0 , 0) position , we find the dot product&amp;nbsp;of &lt;/p&gt;
&lt;div class="math"&gt;$$ 
\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 \\ 
3 &amp;amp; 2 &amp;amp; 2 \\ 
2 &amp;amp; 4 &amp;amp; 3
\end{bmatrix} \text{ * }
\begin{bmatrix} 
-1 &amp;amp; -1 &amp;amp; -1 \\ 
-1 &amp;amp; 8 &amp;amp; -1 \\ 
-1 &amp;amp; -1 &amp;amp;  -1 
\end{bmatrix} = \text{-1}$$&lt;/div&gt;
&lt;p&gt;which essentially is&amp;nbsp;: &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(1\times-1 + 1\times-1 + 1\times-1 + 3\times-1 + 2\times8 + 2\times-1 + 2\times-1 + 4\times-1 + 3\times-1 =&amp;nbsp;-1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similarly, assume an input image - &lt;span class="math"&gt;\(I\)&lt;/span&gt;  is of Size &lt;span class="math"&gt;\((200 \text{ x } 200)\)&lt;/span&gt;, and we have a kernel - &lt;span class="math"&gt;\(k\)&lt;/span&gt; of size &lt;span class="math"&gt;\((10 \text{ x } 10)\)&lt;/span&gt;. The convolution of this kernel over this image is to basically, take the kernel and slide it over the image column by column, row by row. Initially the kernel is placed at &lt;span class="math"&gt;\(I[0, 0]\)&lt;/span&gt;. Therefore the kernel covers a region of &lt;span class="math"&gt;\(10 \text{ x } 10\)&lt;/span&gt; (size of kernel) starting from &lt;span class="math"&gt;\(I[0, 0] \text{ to } I[10,10]\)&lt;/span&gt;. Once the dot product (convolution) is computed over this pixel region, the kernel is shifted by one column to the right. Now the kernel occupies an image region of &lt;span class="math"&gt;\(I[0,1 ] \text{ to } I[10, 11]\)&lt;/span&gt; and so on. 
&lt;center&gt;&lt;img alt="conv_img" src="http://kushalvyas.github.io/images/cnn_images/convolution.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Source: &lt;a href="http://stackoverflow.com/questions/15356153/how-do-convolution-matrices-work" target="_blank"&gt;Stackoverflow : How convolution matrices work&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;A series of convolution operations take place at every layer, which extrapolates the pertinent information from the images. At every layer, multiple convolution operations take place, followed by zero padding and eventually passed through an activation layer and what is outputted is an Activation&amp;nbsp;Map. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Filters and&amp;nbsp;Stride&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An image can be represented as a 3D array. The dimension being &lt;span class="math"&gt;\(rows, cols, depth\)&lt;/span&gt;. The 
&amp;#8216;depth&amp;#8217; refers to the channels of the image, namely &lt;span class="math"&gt;\(\text{red, green and blue}\)&lt;/span&gt;. Hence, a color &lt;span class="caps"&gt;RGB&lt;/span&gt; image of size &lt;span class="math"&gt;\(200 \text{ x } 200\)&lt;/span&gt; is actually of size &lt;span class="math"&gt;\(200 \text{ x } 200 \text{ x } 3\)&lt;/span&gt;. To convolve this image, we need a kernel that not only convolves spatially, i.e along the rows and columns but also reaches out to the values in all the channels. Hence, the kernel used will be of a size &lt;span class="math"&gt;\(k = 10 \text{ x } 10 \text{ x } 3\)&lt;/span&gt;. &lt;strong&gt;Why 3?&lt;/strong&gt; Because this will convolve through the depth of the image. The filter depth must be same as the depth of the input from the previous layer. Each filter can be moved over the image, column by column and row by row -&amp;gt; this means that the stride is 1. Inshort, The number of pixels skipped whilst the filter traverses the image is stride. One can have a filter with stride &amp;#8216;s&amp;#8217;, implying that it&amp;#8217;ll skip &amp;#8216;s&amp;#8217; rows/columns while moving towards the other end of the image, convolving at each&amp;nbsp;step.&lt;/p&gt;
&lt;!-- &lt;center&gt;![conv_layer]({filename}/images/cnn_images/conv_layer.png)&lt;/center&gt;
&lt;center&gt;Source : [Convolutional Neural Nets : Wiki](https://en.wikipedia.org/wiki/File:Conv_layer.png)&lt;/center&gt;

 --&gt;

&lt;p&gt;&lt;strong&gt;Activation and&amp;nbsp;Pooling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An activation function basically defines the output of the neuron for the given input. Take a simple Hebbian Learning example . An activation function will return &lt;span class="math"&gt;\(1\)&lt;/span&gt; if input is &lt;span class="math"&gt;\( &amp;gt; 0\)&lt;/span&gt; and return &lt;span class="math"&gt;\(0\)&lt;/span&gt; otherwise. The value of input being greater that 0, is nothing but a threshold value. Instead of 0, any other value can also act as a threshold. In ConvNets, a the popular activation function used is a Rectifier or &lt;span class="caps"&gt;RELU&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$f(x) = \text{max(0, x)}$$&lt;/div&gt;
&lt;p&gt;This means that the output of the convolution is thresholded at 0. All positive values are returned as is and all negative values are made&amp;nbsp;0. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="relu_plot" src="http://kushalvyas.github.io/images/cnn_images/relu.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;For our above example, if the convolution output is passed to the Rectifier unit, it&amp;#8217;ll set all negative values to&amp;nbsp;zero. &lt;/p&gt;
&lt;p&gt;&lt;img alt="rell image" src="http://kushalvyas.github.io/images/cnn_images/rell.png" /&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, the pooling operation is used to down-sample / reduce the activation map. This essentially reduces the volume of the middle-stages output. The output size is computed the same way as computed in the &lt;span class="caps"&gt;CONV&lt;/span&gt; layer. An important thing to note is that is that during the convolution layer, there is a rapid reduction in the dimensionality of the input. To maintain it at a constant size through multiple &lt;span class="caps"&gt;CONV&lt;/span&gt; layers,zero padding is used. Zero padding will pad the image with zeros on its boundaries making it of the same size as the input. It is in the pooling layer where the size reduction takes&amp;nbsp;place.&lt;/p&gt;
&lt;h2&gt;Implementation with&amp;nbsp;Caffe&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://caffe.berkeleyvision.org/" target="_blank"&gt;Caffe&lt;/a&gt;&lt;/strong&gt; is a framework for deep learning, very popular for its simplicity in implementing &lt;span class="caps"&gt;CNN&lt;/span&gt;&amp;#8217;s. It has been developed by the Berkely Vision &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Learning Center. You can use a &lt;span class="caps"&gt;CPU&lt;/span&gt; as well as a &lt;span class="caps"&gt;GPU&lt;/span&gt; mode. It is widely known that when it comes to matrix and other such image operations , most of which can be done in parallel, &lt;span class="caps"&gt;GPU&lt;/span&gt;&amp;#8217;s triumph over &lt;span class="caps"&gt;CPU&lt;/span&gt;&amp;#8217;s. I used my &lt;span class="caps"&gt;CPU&lt;/span&gt; machine to train a limited Caltech101 dataset, and it went on for 3&amp;nbsp;days. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&amp;nbsp;Up&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The example covers a classification tutorial with Caffe and your own dataset. Before starting off, it is important that Caffe and the following modules are&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.vision.caltech.edu/Image_Datasets/Caltech101/" target="_blank"&gt;Caltech101 limted dataset&lt;/a&gt;&lt;/em&gt; : This comprises of 101 object categories which can be used to test and learn classification. You can download and extract the dataset in the working&amp;nbsp;directory. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.dropbox.com/s/y3k1tnlhymo2xd8/caltechNET_train_iter_356.caffemodel?dl=0" target="_blank"&gt;My Pre-Trained Caltech101 Model&lt;/a&gt;&lt;/em&gt; : Incase you are low on computing power, or would just like to test the code, you can simply download my pretrained&amp;nbsp;network.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparing &lt;span class="caps"&gt;LMDB&lt;/span&gt; format&amp;nbsp;data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This article will now cover how to make a custom dataset , and train it using caffe. There are constraints, wherein caffe uses an lmdb data format, and it is important to convert your dataset into the respective&amp;nbsp;formats.&lt;/p&gt;
&lt;p&gt;Next is converting the images into an lmdb format. The caffe documentation mentions to generate a txt file comprising of the image path with its associated class number. You can write a simple python script listing contents of your training and testing directories into such text&amp;nbsp;files&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sunflower&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0020&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Motorbike&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0033&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;soccer_ball&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0042&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;accordion&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0258&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dollar_bill&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0673&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;path_to_caltech101&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&lt;/span&gt;&lt;span class="n"&gt;limited&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;airplanes&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;image_0050&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
Once done, use the Caffe &lt;code&gt;convert_imageset&lt;/code&gt; to convert these images into the leveldb/lmdb data format. The default backend option is an lmdb backend. The &lt;code&gt;convert_imageset&lt;/code&gt;  can be found at &lt;code&gt;caffe/build/tools&lt;/code&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;convert_imageset&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;resize_height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;resize_width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;PATH_TO_IMAGESET&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;PATH_TO_TEXT_FILE_CRAEATED_ABOVE&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_LMDB_LOCATION&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Or simply change the paths and location inside the &lt;code&gt;caffe/examples/imagenet/create_imagnet.sh&lt;/code&gt; file&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
Eventually your working directory will comprise of a train &lt;span class="caps"&gt;LMDB&lt;/span&gt; directory and a test &lt;span class="caps"&gt;LMDB&lt;/span&gt;&amp;nbsp;directory.    &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;src&lt;/span&gt;
    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;train_lmdb_folder&lt;/span&gt;
    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;test_lmdb_folder&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining the Network Architecture&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Caffe network architectures are very simple to define. Create a file &amp;#8220;model.prototxt&amp;#8221; and define the network architecture as follows. We will be using the AlexNET model (winner of ImageNet challenge 2012).
Given below is a representation of how the net looks&amp;nbsp;like.&lt;/p&gt;
&lt;p&gt;&lt;img alt="net_arch" src="http://kushalvyas.github.io/images/cnn_images/net_arch.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://kushalvyas.github.io/images/cnn_images/net_arch.png" target="_blank"&gt;Click here and zoom to view&amp;nbsp;it &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To define the architecture, open the model.prototxt file. The model begins with a net name&amp;nbsp;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;NameoftheNET&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then sequentially start defining layers. 
First comes the data layer. Any layer that needs to be defined has a few mandatory parameters, that help in the definition of the neural net structure. To begin with, each layer is associated with a type (data, conv, relu, pool, etc) and its location - whether it on top of a previous layer, and beneath the next layer. Similarly, the data layer is serves as the input to the&amp;nbsp;ConvNet. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nl"&gt;name:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
          &lt;span class="nl"&gt;type:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Data&amp;quot;&lt;/span&gt;
          &lt;span class="nl"&gt;top:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
          &lt;span class="nl"&gt;top:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;
          &lt;span class="n"&gt;include&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nl"&gt;phase:&lt;/span&gt; &lt;span class="n"&gt;TRAIN&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="n"&gt;transform_param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
            &lt;span class="n"&gt;mirror&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
            &lt;span class="nl"&gt;crop_size:&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;
            &lt;span class="nl"&gt;mean_file:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;mean.binaryproto&amp;quot;&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="n"&gt;data_param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nl"&gt;source:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;path - to - train_lmdb_folder&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;batch_size:&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;
            &lt;span class="nl"&gt;backend:&lt;/span&gt; &lt;span class="n"&gt;LMDB&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As seen, the next layer is a &lt;code&gt;conv&lt;/code&gt; layer. Defining layers in Caffe is quite straightforward. Each convolutional layer has a number of required and optional parameters. The required parameters involve num_inputs i.e. input size, and the kernel size. Optional parameters comprise of strides, padding width,&amp;nbsp;etc. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nl"&gt;name:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;type:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Convolution&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;bottom:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;top:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;

            &lt;span class="p"&gt;....&lt;/span&gt;
            &lt;span class="p"&gt;....&lt;/span&gt;
            &lt;span class="p"&gt;....&lt;/span&gt;

            &lt;span class="n"&gt;convolution_param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nl"&gt;num_output:&lt;/span&gt; &lt;span class="mi"&gt;96&lt;/span&gt;
                &lt;span class="nl"&gt;kernel_size:&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;
                &lt;span class="nl"&gt;stride:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
                &lt;span class="p"&gt;...&lt;/span&gt;
                &lt;span class="p"&gt;...&lt;/span&gt;
                &lt;span class="p"&gt;...&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Moving on, we talked about Pool and ReLU layers before as an integral part of ConvNets. Here&amp;#8217;s how to define&amp;nbsp;them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;                              &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nl"&gt;name:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;relu1&amp;quot;&lt;/span&gt;                       &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;pool1&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;type:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ReLU&amp;quot;&lt;/span&gt;                        &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Pooling&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;bottom:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;                     &lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;norm1&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;top:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;                        &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;pool1&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;                                       &lt;span class="n"&gt;pooling_param&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
                                                    &lt;span class="nl"&gt;pool:&lt;/span&gt; &lt;span class="n"&gt;MAX&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;MAX&lt;/span&gt; &lt;span class="n"&gt;POOL&lt;/span&gt; &lt;span class="n"&gt;algorithm&lt;/span&gt;
                                                    &lt;span class="n"&gt;kernel_size&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
                                                    &lt;span class="nl"&gt;stride:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
                                                &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
A ReLU will simply activate the convolution layer output and essentially threshold it to 0. (refer Rectifier activation function). Whereas the pooling will reduce the size of the output. AlexNet also uses a normalization layer, which is not much used nowadays.  Similarly, one can keep defining layers in order according to the architecture you want. Lastly, each ConvNet has a fully connected layer, where the input is a column vector ( 1&amp;nbsp;D). &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nl"&gt;name:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;type:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;SoftmaxWithLoss&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;bottom:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;fc8&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;bottom:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;
            &lt;span class="nl"&gt;top:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;You can view the &lt;a href="https://gist.github.com/kushalvyas/31e595bf1fca3a2dd50227ab524427a7" target="_blank"&gt;complete AlexNet architecture :&amp;nbsp;gist&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training your data&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Training can be done in either ways. You can write a python script (which I will update to the blog in a few days)  or you can use the &lt;em&gt;caffe tool&lt;/em&gt; to do so. For now, I&amp;#8217;ll be explaining w.r.t the caffe tool which can be found in &lt;code&gt;$caffe_root_dir/build/tools/caffe&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Once the network architecture has been fixed, all your training lmdb files created the next step is to define the Caffe Solver, defining the learning rate, momentum, solving mode (either &lt;span class="caps"&gt;CPU&lt;/span&gt; or &lt;span class="caps"&gt;GPU&lt;/span&gt;), and path for saving snapshots of the&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="nl"&gt;net:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;model.prototxt&amp;quot;&lt;/span&gt;
        &lt;span class="nl"&gt;base_lr:&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
        &lt;span class="nl"&gt;lr_policy:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;step&amp;quot;&lt;/span&gt;
        &lt;span class="nl"&gt;gamma:&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
        &lt;span class="nl"&gt;stepsize:&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
        &lt;span class="nl"&gt;display:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
        &lt;span class="nl"&gt;max_iter:&lt;/span&gt; &lt;span class="mi"&gt;4500&lt;/span&gt;
        &lt;span class="nl"&gt;momentum:&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;
        &lt;span class="nl"&gt;weight_decay:&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;
        &lt;span class="nl"&gt;snapshot:&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
        &lt;span class="nl"&gt;snapshot_prefix:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;snapshots1/caltechNET_train&amp;quot;&lt;/span&gt;
        &lt;span class="nl"&gt;solver_mode:&lt;/span&gt; &lt;span class="n"&gt;CPU&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
Make sure that the solver contains the correct name of the neural net&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Computing Image means&lt;/em&gt; : &lt;a href="http://caffe.berkeleyvision.org/gathered/examples/imagenet.html" target="_blank"&gt;The AlexNet model requires to subtract each image instance from the mean&lt;/a&gt;. You can refer to the &lt;code&gt;caffe_root_dir/examples/imagenet/make_imagenet_mean.sh&lt;/code&gt; file to compute the&amp;nbsp;mean.&lt;/p&gt;
&lt;p&gt;The  model, solver, means file and lmdb imagesets have been made. Next, is to sit and train the model. This can be done using the caffe executable&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$caffe_root_dir/build/tools/caffe train --solver=solver.prototxt&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Literally, sit back and enjoy ! Your snapshots directory will get updated with the model snapshots as and&amp;nbsp;when.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;The final trained net is available in 
&lt;code&gt;snapshots/&amp;lt;file_iter_number&amp;gt;.caffemodel&lt;/code&gt; file. There are a few tricks to test a particular&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;Firstly, it is important that we map the training classes to their respective class names. If you see above, in the part where the data was being prepared, we made a txtfile of the following&amp;nbsp;pattern&lt;/p&gt;
&lt;p&gt;&amp;lt;path_to_caltech101&gt;/limited/train/sunflower/image_0020.jpg&amp;nbsp;0&lt;/p&gt;
&lt;p&gt;The class &amp;#8216;sunflower&amp;#8217; has been mapped to 0. and so on with other classes.
&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class name&lt;/th&gt;
&lt;th&gt;Mapped to class number&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;sunflower&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Motorbike&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dollar_bill&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;airplanes&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;soccer_ball&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faces&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;accordion&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
Next, the caffe model needs to be&amp;nbsp;loaded. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;caffe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;path_to_model.prototxt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;snapshot_file.caffemodel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                       &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;means.npy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                       &lt;span class="n"&gt;channel_swap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                       &lt;span class="n"&gt;raw_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="n"&gt;image_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;caffe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_image&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;path_to_image&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;im&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;class_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c"&gt;# this returns the class id or the class number&lt;/span&gt;

&lt;span class="o"&gt;...&lt;/span&gt; 
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;classname&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;class_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c"&gt;# to print the name of the class&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Some tricks&lt;/strong&gt;&amp;nbsp;: &lt;/p&gt;
&lt;p&gt;There may be some cases where the means file is stored as a means.binaryproto. There is a quick way to convert it to a npy file. &lt;a href="https://github.com/BVLC/caffe/issues/808" target="_blank"&gt;Refer issue #808&lt;/a&gt; for the conversion of .binaryproto to .npy files.
&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="c"&gt;# solution github issue 808 by @mafiosso&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; 
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;blob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;caffe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;caffe_pb2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BlobProto&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mean.binaryproto&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;blob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ParseFromString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;blob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; 
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; 
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# size of image. &lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;means.npy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
It is highly recommended that you use &lt;span class="caps"&gt;GPU&lt;/span&gt;&amp;#8217;s to tarin networks. However, feel free to experiment with your personal computers. Works just fine&amp;nbsp;!&lt;/p&gt;
&lt;h2&gt;Classification&amp;nbsp;results&lt;/h2&gt;
&lt;p&gt;The Caltech101 dataset limited directory already has a split of data into train and&amp;nbsp;test. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;p&gt;There are a few snaps of outputs when the model was tested on the limited version of the dataset&amp;nbsp;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="im1" src="http://kushalvyas.github.io/images/cnn_images/new_a.png" /&gt;
&lt;img alt="im2" src="http://kushalvyas.github.io/images/cnn_images/new_airplane.png" /&gt;
&lt;img alt="im3" src="http://kushalvyas.github.io/images/cnn_images/new_soccer_ball.png" /&gt;&lt;br&gt;
&lt;img alt="im4" src="http://kushalvyas.github.io/images/cnn_images/new_motorbike.png" /&gt;
&lt;img alt="im5" src="http://kushalvyas.github.io/images/cnn_images/new_sunflower.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;So, we have completed the tutorial on how to create a custom dataset and train it using caffe. Now you can implement this and train any dataset you want. I would recommend reading up on the references to get a better understanding of&amp;nbsp;ConvNets.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;strong&gt;I will keep updating this article with newly pretrained models and adding more about python interfacing with Caffe. Till then, have fun implementing &lt;span class="caps"&gt;CNN&lt;/span&gt;&amp;#8217;s.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href="http://cs231n.github.io/convolutional-networks/" target="_blank"&gt;Convolutional Neural Networks for Visual Recognition - CS231n&amp;nbsp;Stanford&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href="http://caffe.berkeleyvision.org" target="_blank"&gt;Berkely Vision Lab -&amp;nbsp;Caffe&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href="http://caffe.berkeleyvision.org/gathered/examples/imagenet.html" target="_blank"&gt;ImageNet tutorial - Caffe&amp;nbsp;Docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank"&gt;AlexNet&amp;nbsp;Paper&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="ObjectRecognition"></category></entry><entry><title>ImageÂ Stitching A SimplisticÂ Tutorial</title><link href="http://kushalvyas.github.io/stitching.html" rel="alternate"></link><updated>2016-09-25T18:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-09-25:stitching.html</id><summary type="html">&lt;h3&gt;Multiple Image&amp;nbsp;Stitching&lt;/h3&gt;
&lt;p&gt;I must say, even I was enjoying while developing this tutorial . Something about image perspective and enlarged images is simply captivating to a computer vision student (&lt;span class="caps"&gt;LOL&lt;/span&gt;) . I think, image stitching is an excellent introduction to the coordinate spaces and perspectives vision. Here I am going to show how to take an ordered set of many images, &lt;strong&gt;(assuming they have been shot from left to right direction)&lt;/strong&gt;&amp;nbsp;.&lt;/p&gt;
&lt;p&gt;So what is image stitching ? In simple terms, for an input group of images, the output is a composite image such that it is a culmination of scenes. At the same time, the logical flow between the images must be&amp;nbsp;preserved.&lt;/p&gt;
&lt;p&gt;For example, consider the set of images below. (Taken from &lt;a href="https://www.mathworks.com/examples/matlab-computer-vision/mw/vision_product-FeatureBasedPanoramicImageStitchingExample-feature-based-panoramic-image-stitching"&gt;matlab examples&lt;/a&gt;).
From a group of an input montage, we are essentially creating a singular stitched image. One that explains the full scene in detail. It is quite an interesting algorithm !
&lt;center&gt;
&lt;img src="images/stitching/matlab_montage.png" width="400" height="300" &gt;
&lt;img src="images/stitching/matlab_pano.png" width="400" height="300"&gt;&lt;br&gt;
&lt;caption&gt;(Taken from &lt;a href="https://www.mathworks.com/examples/matlab-computer-vision/mw/vision_product-FeatureBasedPanoramicImageStitchingExample-feature-based-panoramic-image-stitching"&gt;matlab examples&lt;/a&gt;).&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Topics being covered&lt;/strong&gt;
The implementation will be carried out in python programming&amp;nbsp;language. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading multiple images ( in&amp;nbsp;order)&lt;/li&gt;
&lt;li&gt;Finding logical consistencies within images (this will be done using&amp;nbsp;homography).&lt;/li&gt;
&lt;li&gt;Stitching Up&amp;nbsp;images. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In need for any literature reference, please refer this &lt;a href="http://matthewalunbrown.com/papers/ijcv2007.pdf"&gt;paper by Mathew&amp;nbsp;Brown&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Let&amp;#8217;s begin&amp;nbsp;&amp;#8230;&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s first understand the concept of mosaicking or image stitching. Basically if you want to capture a big scene. Now your camera can only provide an image of a specific resolution and that resolution , say 640 by 480 , is certainly not enough to capture the big panoramic view. So , what one can do is capture multiple images of the entire scene and then put together all bits and pieces into one big mat of images. Yes, it seems good .. right ! Such photographs , which pose as an ordered collection of a scene are called as mosaics or panoramas. The entire process of acquiring multiple image and converting them into such panoramas is called as image mosaicking. And finally, we have one beautiful big and large photograph of the scenic&amp;nbsp;view.&lt;/p&gt;
&lt;p&gt;Another method for achieving this, is by using wide angle lens in your camera. What a wide angle lens does, is effectively increase your field of view. The output, will differ (obviously). But for the purposes of this tutorial, let&amp;#8217;s get into how to create panoramas using computers and not lens&amp;nbsp;:P &lt;/p&gt;
&lt;h3&gt;Setting up the&amp;nbsp;environment&lt;/h3&gt;
&lt;p&gt;Please note that your system is setup with Python 2.7 (Code implementation is in python2.7 if you have other versions, please modify the code accordingly) and OpenCV 3.0 . We will be using OpenCV&amp;#8217;s helper utilities for reading images, writing images and conversion of color spaces. Once the images are obtained, the entire computation of the panorama will be done using a home brewed function.  This blog article is divided into three major&amp;nbsp;parts. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt; Input, read and process images : image paths from text files. Each textfile contains the list of paths to each image. &lt;strong&gt;Make sure that the paths are in left_to_right order of&amp;nbsp;orientation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt; Computation relative orientation of images w.r.t each other :&amp;nbsp;pairwise&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt; The stitching / mix and match module : which essentially joins the two images at a&amp;nbsp;time&lt;/p&gt;
&lt;h3&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The algorithm for performing image stitching is pretty&amp;nbsp;straightforward.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;--&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;
    &lt;span class="n"&gt;Assuming&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;no_of_images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;let&lt;/span&gt; &lt;span class="n"&gt;centerIdx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;positions&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;centerIdx&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;perform&lt;/span&gt; &lt;span class="n"&gt;leftward&lt;/span&gt; &lt;span class="n"&gt;stitching&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;positions&lt;/span&gt; &lt;span class="n"&gt;centerIdx&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;perform&lt;/span&gt; &lt;span class="n"&gt;rightward&lt;/span&gt; &lt;span class="n"&gt;stitching&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The output will be a complete mosaic of the input&amp;nbsp;images.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some Constraints&lt;/strong&gt;
The algorithm is time consuming, due to the number of iterations involved, it is best that hte input number of images is not too high or not of very high resolution (eg. 4000x3000). 
My implementation is based on a 2 &lt;span class="caps"&gt;GB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; computer having intel i3 processor (Not tested it on my machine yet !). Feel free to upgrade/scale this model using higher specs, or maybe &lt;span class="caps"&gt;GPU&lt;/span&gt;&amp;#8217;s .It&amp;#8217;s never too late to&amp;nbsp;try.&lt;/p&gt;
&lt;h3&gt;Project Architecture&amp;nbsp;:&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;-|&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt;        &lt;span class="o"&gt;|--&lt;/span&gt; &lt;span class="n"&gt;pano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt;        &lt;span class="o"&gt;|--&lt;/span&gt; &lt;span class="n"&gt;txtlists&lt;/span&gt;&lt;span class="o"&gt;-|&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt;                     &lt;span class="o"&gt;|--&lt;/span&gt;&lt;span class="n"&gt;files1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt; &lt;span class="p"&gt;....&lt;/span&gt; 
    &lt;span class="o"&gt;|&lt;/span&gt;   
    &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt;           &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;img1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt;           &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;abc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; 
    &lt;span class="o"&gt;|&lt;/span&gt;           &lt;span class="p"&gt;....&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt; &lt;a href="https://github.com/kushalvyas/Python-Multiple-Image-Stitching"&gt;Click here to check out the code&lt;/a&gt; on &lt;a href="https://github.com/kushalvyas" target="_blank"&gt;Github&lt;/a&gt; &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The project architecture is as follows. The code directory contains the main &lt;code&gt;pano.py&lt;/code&gt; file. Also it contains a &lt;code&gt;txtlists/&lt;/code&gt; directory which contains files having the paths to images in the panorama. These images are individually stored inside &lt;code&gt;images/ directory&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial, I will be using images from &lt;a href="http://www.cvl.isy.liu.se/en/research/datasets/passta/" target="_blank"&gt;&lt;span class="caps"&gt;PASSTA&lt;/span&gt; Datasets&lt;/a&gt;. It contains 2 datasets, Lunchroom and Synthetic.
Also, to test, I will be using images from &lt;a href="http://study.marearts.com/2013/11/opencv-stitching-example-stitcher-class.html" target="_blank"&gt;Mare&amp;#8217;s Computer Vision blog&lt;/a&gt;. . I&amp;#8217;ll be using &lt;a href="https://github.com/daeyun/Image-Stitching/tree/master/img/hill" target="_blank"&gt;daeyun&amp;#8217;s&lt;/a&gt; test hill images as well. and &lt;a href=""&gt;tsherlock&lt;/a&gt; too !! (&lt;strong&gt;For respective usage and citations , take a look at the references&lt;/strong&gt;)&lt;/p&gt;
&lt;h3&gt;Now the real&amp;nbsp;part.&lt;/h3&gt;
&lt;p&gt;To understand either of the leftward stitching or rightward stitching module, first let&amp;#8217;s get some vision concepts straight. They&amp;nbsp;being: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;*Homography : Oh I love this&amp;nbsp;!&lt;/li&gt;
&lt;li&gt;*Warping : Cause, without warping, homography would feel a bit&amp;nbsp;lonely&lt;/li&gt;
&lt;li&gt;*Blending : Cause intensity differences are a bit too&amp;nbsp;mainstream&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Homography&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Okay, so assume you&amp;#8217;re looking at a scenery. You will be having a field of view and that field of view is of what you want to make a panorama. You can rotate your head and cover a big area. But while you are looking straight, looking directly perpendicularly at a sub-scene, the remaining part of the scenery appears slightly inclined or slightly narrowed out. This is due to simple physics. Since you are facing in one direction, the things to your extreme periphery appears unclear, reduced in dimension and not necessarily straight/normal (a bit inclined). This is exactly what we will be&amp;nbsp;exploiting.&lt;/p&gt;
&lt;p&gt;Consider the images shown in the above figure. Every image will contain some common portion with the other images. Due to this commonness we are able to say that &lt;span class="math"&gt;\(image \text{ x}\)&lt;/span&gt; will either lie on to the right or left side of &lt;span class="math"&gt;\(image \text{ y }\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;Anyway, now that I&amp;#8217;ve made that clear, let&amp;#8217;s proceed as to how do we calculate homography. Say you have a pair of images &lt;span class="math"&gt;\(I1 \text{ , } I2\)&lt;/span&gt; . You capture the first image. Then you decide to rotate your camera, or maybe perform some translation  or maybe a combination of rotation / translation motion. Then having update your new camera position , you capture a second image. The problem now at hand is, How do you solve for a system wherein you&amp;#8217;re required to create a transformation that efficiently maps a point that is being projected in both the images. Or in simple terms, How do you visualize one image w.r.t another point of view, given there is some information available about both your points of&amp;nbsp;views.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="images/stitching/per.png" width="400" height="300" &gt;
&lt;br&gt;
&lt;caption&gt;Types of transforms&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Each of the above transformations performs some sorts of image transformation. For eg. a projective (well in your case,homography ) transform will preserve straight lines, .. etc. Moving on, a homography matrix is such that , if applied to any image, transforms image plane P1 to another image plane&amp;nbsp;P2.&lt;/p&gt;
&lt;p&gt;See the pic below, you&amp;#8217;ll understand what iâm talking about.
&lt;center&gt;
&lt;img src="images/stitching/mix.png"&gt;
&lt;br&gt;
&lt;caption&gt;Source :&lt;a href="http://stackoverflow.com/questions/13570140/how-to-use-homography-to-transform-pictures-in-opencv"&gt;This &lt;span class="caps"&gt;SO&lt;/span&gt; Post&lt;/a&gt; &lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;What you see above, can be an output of applying homography from &lt;span class="math"&gt;\(I1=H \times I2\)&lt;/span&gt; . &lt;a href="http://stackoverflow.com/questions/13570140/how-to-use-homography-to-transform-pictures-in-opencv"&gt;&lt;span class="caps"&gt;HOW&lt;/span&gt; &lt;span class="caps"&gt;TO&lt;/span&gt; use Homography&lt;/a&gt; to transform pictures in OpenCV? ( Check out this answer too. these pics have been taken from the aforementioned post&amp;nbsp;).&lt;/p&gt;
&lt;p&gt;You can use &lt;a href="http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography"&gt;opencv findhomography ( )&lt;/a&gt; method to solve for homography. For finding &lt;span class="math"&gt;\(I1=H \times I2\)&lt;/span&gt; you will need to pass coordinates of points in original image 1 plane and coordinates of target points in image 2 to the method. Once through, the method will spit out the homography&amp;nbsp;matrix&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to identify points to calculate homography&amp;nbsp;!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the straight forward methods is as&amp;nbsp;follows&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;similar&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;both&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;

    &lt;span class="n"&gt;Out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;them&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;good&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;you&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="n"&gt;plenty&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;tutorials&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;these&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;Make&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt; &lt;span class="n"&gt;sorts&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;featuresofI1&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;srcPoints&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;featuresofI2&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dstPoints&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;opencv&lt;/span&gt; &lt;span class="n"&gt;nomenclature&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;Homography&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;RANSAC&lt;/span&gt; &lt;span class="n"&gt;algorithm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;strong&gt; Step 1: Feature extraction&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We shall be using opencv_contrib&amp;#8217;s &lt;span class="caps"&gt;SIFT&lt;/span&gt; descriptor. &lt;span class="caps"&gt;SIFT&lt;/span&gt; , as in Scale Invariant Feature Transform, is a very powerful &lt;span class="caps"&gt;CV&lt;/span&gt; algorithm. Please read my &lt;a href="http://kushalvyas.github.io/BOV.html"&gt;Bag of Visual Words for Image classification post&lt;/a&gt; to understand more about features.
Also, check out &lt;a href="http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html"&gt;OpenCV&amp;#8217;s docs on &lt;span class="caps"&gt;SIFT&lt;/span&gt;&lt;/a&gt;. They are a pretty good resource as&amp;nbsp;well! &lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="n"&gt;sift_obj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xfeatures2d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SIFT_create&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;descriptors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keypoints&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sift_obj&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detectAndCompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image_gray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;If you plot the features, this is how it will look .
(Image on left shows actual image. Image on the right is annotated with features detected by &lt;span class="caps"&gt;SIFT&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;Example 1 : using Lunchroom image
&lt;center&gt;
&lt;img src="images/stitching/sift_keypoints101.jpg"  &gt;
&lt;br&gt;
&lt;caption&gt;Lunchroom image : &lt;span class="caps"&gt;PASSTA&lt;/span&gt; Dataset&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;!-- Example 2 : using building image
&lt;center&gt;
&lt;img src="images/stitching/building_sift.png" width="400" height="300" &gt;
&lt;/center&gt; --&gt;

&lt;p&gt;&lt;strong&gt; Step 2: Matching correspondences between images &lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Once you have got the descriptors and keypoints of 2 images, i.e. an image pair, we will find correspondences between them. Why do we do this ? Well, in order to join any two images into a bigger images, we must obtain as to what are the overlapping points. These overlapping points will give us an idea of the orientation of the second image w.r.t to the other one. And based on these common points, we get an idea whether the second image has just slid into the bigger image  or has it been rotated and then overlapped, or maybe scaled down/up and then fitted. All such information is yielded by establishing correspondences. This process is called &lt;strong&gt;registration&lt;/strong&gt;&amp;nbsp;.&lt;/p&gt;
&lt;p&gt;For matching, one can use either &lt;span class="caps"&gt;FLANN&lt;/span&gt; or BFMatcher, that is provided by&amp;nbsp;opencv. &lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="c"&gt;# FLANN parameters&lt;/span&gt;
    &lt;span class="n"&gt;FLANN_INDEX_KDTREE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;index_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;algorithm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FLANN_INDEX_KDTREE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trees&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;search_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c"&gt;# or pass empty dictionary&lt;/span&gt;

    &lt;span class="n"&gt;flann&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FlannBasedMatcher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;search_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flann&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;knnMatch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;des1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;des2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;img3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drawMatchesKnn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img1c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;kp1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;img2c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;kp2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;matches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;draw_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;correspondences&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;waitKey&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Having computed the matches, you get a similar output&amp;nbsp;: &lt;/p&gt;
&lt;p&gt;With the Lunchroom dataset
&lt;center&gt;
&lt;img src="images/stitching/matches12.png" &gt; 
&lt;br&gt;
&lt;caption&gt;Feature matching on lunchroom images&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;With the Hill example:
&lt;center&gt;
&lt;img src="images/stitching/img31.png" width="400" height="300" &gt;
&lt;br&gt;
&lt;caption&gt;Feature matching on hill example image&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;!-- With the building example:
&lt;center&gt;
&lt;img src="images/stitching/img3.png" width="400" height="300" &gt;
&lt;/center&gt; --&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Compute&amp;nbsp;Homography&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yes, once we have obtained matches between the images, our next step is to calculate the homography matrix. As described above, the homography matrix will use these matching points, to estimate a relative orientation transform within the two images.
i.e. it&amp;#8217;ll solve for the equation
&lt;center&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$I_x = H \times I_y $$&lt;/div&gt;
&lt;p&gt; &lt;/center&gt;
Hence, it solves for the matrix &lt;span class="math"&gt;\(H\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Well,  to estimate the homography is a simple task. If you are using opencv, it&amp;#8217;s a two line code. However , I&amp;#8217;d recommend that one implements&amp;nbsp;oneself.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findHomography&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;srcPoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dstPoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RANSAC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Viola ! Our homography matrix looks something like this&amp;nbsp;&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
    &lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{bmatrix}
h_\text{11} &amp;amp; h_\text{12}  &amp;amp; h_\text{13} \\ 
h_\text{21} &amp;amp; h_\text{22} &amp;amp; h_\text{23}  \\ 
h_\text{31} &amp;amp; h_\text{32}   &amp;amp; h_\text{33}
\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Anyway, putting the pretty graphics aside, understand what the homography matrix is . Homography preserves the straight lines in an image. Hence the only possible transformations possible are translations, affines, etc. For example, for an affine&amp;nbsp;transform, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix} h_\text{31} &amp;amp; h_\text{32}   &amp;amp; h_\text{33} \end{bmatrix} = \text{[0 0 1]}$$&lt;/div&gt;
&lt;p&gt;
Also, you can play around with &lt;span class="math"&gt;\(h_\text{13} \text{ and } \  h_\text{23}\)&lt;/span&gt; for&amp;nbsp;translation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Step 4 : Warping &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Stitching &lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;To understand stitching, I&amp;#8217;d like to recommend &lt;a href="http://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/" target="_blank"&gt;Adrian Rosebrock&amp;#8217;s blog post on OpenCV Panorama stitching&lt;/a&gt;. His blog provides a wonderful explanation as to how to proceed with image stitching and panorama construction using 2&amp;nbsp;images.&lt;/p&gt;
&lt;p&gt;So , once we have established a homography, i.e. we know how the second image (let&amp;#8217;s say the image to the right) will look from the current image&amp;#8217;s perspective, we need to transform it into a new space. This transformation mimics the phenomenon that we undergo. That is, the slightly distorted, and altered image that we see from our periphery . This process is called warping. We are converting an image, based on a new transformation. In this case, Im using a planar warping. What I&amp;#8217;m doing, is essentially change the plane of my field of view. Whereas, the &amp;#8220;panorama apps&amp;#8221; use something called as a Cylindrical and spherical warps&amp;nbsp;! &lt;/p&gt;
&lt;p&gt;Types of warping&amp;nbsp;: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;*Planar : wherein every image is an element of a plane surface, subject to translation and rotations&amp;nbsp;&amp;#8230; &lt;/li&gt;
&lt;li&gt;*Cylindrical : wherein every image is represented as if the coordinate system was cylindrical. and the image was plotted on the curved surface of the&amp;nbsp;cylinder.&lt;/li&gt;
&lt;li&gt;*Spherical : the above appends, instead of a cylinder, the reference model is a&amp;nbsp;sphere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each model has its&amp;#8217; own application. For the purposes of this tutorial, I&amp;#8217;ll stick to planar homography and&amp;nbsp;warping.&lt;/p&gt;
&lt;p&gt;So , to warp, essentially change the field of view, we apply the homography matrix to the&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;warped_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;warpPerspective&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;homography_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dimension_of_warped_image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is some visualization  &amp;#8230;.  below are left warped and right warped images Note the orientation and projective-ness of each&amp;nbsp;image. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="images/stitching/merge123.png"  &gt;
&lt;br&gt;
&lt;caption&gt;Warping Images of Lunchroom !&lt;/caption&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;!-- 
&lt;center&gt;
&lt;img src="images/stitching/merge12.jpg"  &gt;
&lt;/center&gt; --&gt;

&lt;p&gt;&lt;strong&gt;Stitching &amp;#8216;em up&amp;nbsp;! &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once, we have obtained a warped image, we simply add the warped image along with the second image. Repeat this over through leftward stitching and rightward stitching, and viola! We have our&amp;nbsp;output.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll get  a bit deeper as to how to perform the image joining part. Say, we have a homography matrix &lt;span class="math"&gt;\(H\)&lt;/span&gt; . If the starting coordinate of each image is &lt;span class="math"&gt;\((0 ,0)\)&lt;/span&gt; and end point is &lt;span class="math"&gt;\((r_e, c_e)\)&lt;/span&gt; , we can get the new warped image dimension by &lt;span class="math"&gt;\(start_p = H \times [0, 0]\)&lt;/span&gt; uptill &lt;span class="math"&gt;\(end_p = H \times [r_e, c_e]\)&lt;/span&gt;. &lt;strong&gt;Note : If start_pt comes out to be negative&lt;/strong&gt; , account for a translational shift. i.e. &amp;#8220;perform translation shift to the image by &lt;span class="math"&gt;\( |start_p |\)&lt;/span&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;. Also make sure that the homography matrix normalized such that the last row amounts to a unit&amp;nbsp;vector.&lt;/p&gt;
&lt;p&gt;You can checkout the above mentioned explanation below 
This is the implementation snippet from the actual code. Please look at the full code to understand in accordance with the&amp;nbsp;post.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;leftstitch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c"&gt;# self.left_list = reversed(self.left_list)&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matcher_obj&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Homography is : &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
            &lt;span class="n"&gt;xh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Inverse Homography :&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xh&lt;/span&gt;
            &lt;span class="c"&gt;# start_p is denoted by f1&lt;/span&gt;
            &lt;span class="n"&gt;f1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;f1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="c"&gt;# transforming the matrix &lt;/span&gt;
            &lt;span class="n"&gt;xh&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;xh&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;ds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;offsety&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;offsetx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="c"&gt;# dimension of warped image&lt;/span&gt;
            &lt;span class="n"&gt;dsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;offsetx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;offsety&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;image dsize =&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dsize&lt;/span&gt;
            &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;warpPerspective&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dsize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c"&gt;# cv2.imshow(&amp;quot;warped&amp;quot;, tmp)&lt;/span&gt;
            &lt;span class="c"&gt;# cv2.waitKey()&lt;/span&gt;
            &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;offsety&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;offsety&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offsetx&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;offsetx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Another method : &lt;/strong&gt; 
There is another method, i.e. using &lt;code&gt;basic for looping constructs&lt;/code&gt; and overlay the two images.
The logic is simple. Input to the method will be the steadyimage and warpedImage. Iterate through both images, and if pixels are equal, put pixel as that value. else give preference to a non black pixel&amp;nbsp;.. &lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mix_match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;i1y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i1x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;i2y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i1x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i1y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt;  \
                        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))):&lt;/span&gt;
                        &lt;span class="c"&gt;# print &amp;quot;BLACK&amp;quot;&lt;/span&gt;
                        &lt;span class="c"&gt;# instead of just putting it with black, &lt;/span&gt;
                        &lt;span class="c"&gt;# take average of all nearby values and avg it.&lt;/span&gt;
                        &lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])):&lt;/span&gt;
                            &lt;span class="c"&gt;# print &amp;quot;PIXEL&amp;quot;&lt;/span&gt;
                            &lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                                &lt;span class="n"&gt;bl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;gl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;rl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;                               
                                &lt;span class="n"&gt;warpedImage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;gl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;rl&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;pass&lt;/span&gt;
        &lt;span class="c"&gt;# cv2.imshow(&amp;quot;waRPED mix&amp;quot;, warpedImage)&lt;/span&gt;
        &lt;span class="c"&gt;# cv2.waitKey()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;warpedImage&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;
But this method will iterate over soooo many pixels. .. It&amp;#8217;s very slow, for two reasons. Firstly , it involves heavy iteration. And, well, I&amp;#8217;d personally execute such heavy loops in C++ and not&amp;nbsp;python. &lt;/p&gt;
&lt;p&gt;So, basically, this is how my main function looks &amp;#8230; the heart and core of all&amp;nbsp;implementations&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;txtlists/files1.txt&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;finally&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Parameters : &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Stitch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leftshift&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c"&gt;# s.showImage(&amp;#39;left&amp;#39;)&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rightshift&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;done&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;test.jpg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leftImage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;image written&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;destroyAllWindows&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Implementation details : Check out the &lt;a href="https://github.com/kushalvyas/Python-Multiple-Image-Stitching"&gt;code&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;Results&amp;nbsp;!!!&lt;/h3&gt;
&lt;p&gt;Well, I&amp;#8217;ve run the code on following pictures ..
You can click as well as test on other sources which deal in panorama and image&amp;nbsp;stitching.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="images/stitching/lunchroom_ultimate.jpg" &gt;&lt;br&gt;
&lt;caption&gt;Stitching with Lunchroom dataset&lt;/caption&gt;
&lt;br&gt;&lt;br&gt;
&lt;img src="images/stitching/444.png"&gt;&lt;br&gt;
&lt;img src="images/stitching/wd123.jpg" &gt;&lt;br&gt;
&lt;caption&gt;Stitching with my house&amp;#8217;s dining table :P&lt;/caption&gt;
&lt;br&gt;&lt;br&gt;
&lt;img src="images/stitching/test12345678.jpg" &gt;&lt;br&gt;
&lt;caption&gt;Stitching with synthetic dataset&lt;/caption&gt;
&lt;br&gt;&lt;br&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Below are other images taken from github and various online sources. View references for more.
&lt;center&gt;
&lt;img src="images/stitching/333.png"&gt;&lt;br&gt;
&lt;img src="images/stitching/test.jpg" &gt;&lt;br&gt;
&lt;caption&gt;Stitching with building example&lt;/caption&gt;
&lt;br&gt;&lt;br&gt;
&lt;img src="images/stitching/222.png"&gt;&lt;br&gt;
&lt;img src="images/stitching/test12.jpg"&gt;&lt;br&gt;
&lt;caption&gt;Stitching using Hill example&lt;/caption&gt;
&lt;br&gt;&lt;br&gt;
&lt;img src="images/stitching/111.png"&gt;&lt;br&gt;
&lt;img src="images/stitching/test1.jpg" &gt;&lt;br&gt;
&lt;caption&gt;Stitching using room example&lt;/caption&gt;
&lt;br&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt; &lt;a href="https://github.com/kushalvyas/Python-Multiple-Image-Stitching"&gt;Click here to check out the code&lt;/a&gt; on &lt;a href="https://github.com/kushalvyas"&gt;Github&lt;/a&gt; &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ll cover cylindrical warping and how opencv actually implements stitching in a different post.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;References&amp;nbsp;:&lt;/h3&gt;
&lt;p&gt;Base paper for panorama using scale invariant features&amp;nbsp;:&lt;/p&gt;
&lt;p&gt;[1] &amp;#8220;Automatic Panoramic Image Stitching using Invariant Features&amp;#8221;, Download.springer.com, 2016. [Online]. Available:&amp;nbsp;matthewalunbrown.com/papers/ijcv2007.pdf&lt;/p&gt;
&lt;p&gt;Test images taken from&amp;nbsp;:&lt;/p&gt;
&lt;p&gt;[2]&amp;#8221;&lt;span class="caps"&gt;PASSTA&lt;/span&gt; Datasets&amp;#8221;, Cvl.isy.liu.se, 2016. [Online]. Available:&amp;nbsp;http://www.cvl.isy.liu.se/en/research/datasets/passta/.&lt;/p&gt;
&lt;p&gt;[3] &amp;#8220;OpenCV Stitching example (Stitcher class, Panorama)&amp;#8221;, Study.marearts.com, 2013. [Online]. Available:&amp;nbsp;http://study.marearts.com/2013/11/opencv-stitching-example-stitcher-class.html.&lt;/p&gt;
&lt;p&gt;[4] &amp;#8220;Github daeyun Image-Stitching Test Images&amp;#8221;, 2016. [Online]. Available:&amp;nbsp;https://github.com/daeyun/Image-Stitching/tree/master/img/hill. &lt;/p&gt;
&lt;p&gt;[5] &amp;#8220;Github tsherlock Test Images&amp;#8221;, 2016. [Online]. Available: .&amp;nbsp;https://github.com/tsherlock/panorama/&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="CV"></category><category term="images"></category><category term="stitching"></category><category term="mosaicking"></category></entry><entry><title>Solving 8 Queens using Genetic Algorithms -Â Evolution</title><link href="http://kushalvyas.github.io/gen_8Q.html" rel="alternate"></link><updated>2016-08-18T18:20:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-08-18:gen_8Q.html</id><summary type="html">&lt;h1&gt;The 8 Queens Problem : An&amp;nbsp;Introduction&lt;/h1&gt;
&lt;p&gt;8 queens is a classic computer science problem. To find possible arrangements of 8 queens on a standard &lt;span class="math"&gt;\(8\)&lt;/span&gt; x &lt;span class="math"&gt;\(8\)&lt;/span&gt; chessboard such that no queens every end up in an attacking configuration. Now, if one knows the basics of chess, one can say that a queen can travel either horizontally, vertically, or diagonally. Hence, for 2 or more queens to be in an attacking state, they have to lie either horizontally or vertically or diagonally in-line with the other queen. The figure on the left illustrates that what all are the attacking positions with respect to a queen. As seen, the attacking positions are those where in the queen can move either horizontally, vertically, or diagonally along the chessboard.&amp;#8221; &lt;span class="math"&gt;\(OK\)&lt;/span&gt; &amp;#8221; resembles the chessboard square which is considered a safe and non-attacking position. Whereas the one marked with an &amp;#8221; &lt;span class="math"&gt;\(X\)&lt;/span&gt; &amp;#8221; is an attacking placement. The figure on right, shows one of the possible arrangements that serve as a solution to the 8 queens&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="attacking_queen" src="http://kushalvyas.github.io/images/queen_attacking.png" /&gt; &amp;nbsp; &amp;nbsp;
&lt;img alt="solution_queen" src="http://kushalvyas.github.io/images/solution.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Images taken from chegg.com and indiamedic&lt;/i&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;There are various methods to solve the 8 queens problem. The most common being &lt;strong&gt;BackTracking&lt;/strong&gt;. It can also be solved using a variety of approaches such as as Hill climbing, Genetic Algorithms - evolution, etc. In this post, I&amp;#8217;ll explain how we approach 8 queens problem using &lt;strong&gt;Genetic Algorithms - Evolution&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;First,  a bit of&amp;nbsp;Biology&amp;#8230;&lt;/h1&gt;
&lt;p&gt;Yea.. I know,. Why Biology ?  Well, get used to the hard truth. Most of our intelligence algorithms are made by ripping off nature. Be it neural networks or simple fibonacci sequence, our great mathematical models are merely a derivative of nature. But this does not make it any less valuable. Rather as my friend says, &amp;#8220;Math is the universal language&amp;#8221;. (I hope he reads this article) So yea, a bit of&amp;nbsp;biology.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s begin with basic cells and their multiplication and some&amp;nbsp;genetics.&lt;/p&gt;
&lt;p&gt;Consider 2 cells. Each cell has something called as pairs of chromosomes. A chromosome is what carries the genetic traits. The pairs and number of chromosomes isin&amp;#8217;t much of relevance. What is important is how the process of cell production causes the genes to propagate into an offspring. This method is called as meiosis. Well, there&amp;#8217;s another way called as mitosis, but in mitosis, there is no genetic diversity that is being propagated (Explained later in post). Till now we have established that the cells contain pairs of chromosomes. And that 2 cells are required for essential cell&amp;nbsp;division. &lt;/p&gt;
&lt;p&gt;Given below is an illustration as to how the division takes place. (Image taken from&amp;nbsp;askabiologist.com)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="celldv" src="http://kushalvyas.github.io/images/meiosis.png" /&gt;
&lt;/center&gt;
&lt;br&gt;
&lt;center&gt;
Process of meiosis.
&lt;img alt="celldv1" src="http://kushalvyas.github.io/images/crossover.png" /&gt;&lt;/p&gt;
&lt;p&gt;Chromosme crossover
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crossover - In Detail&lt;/strong&gt;: When 2 cells meet, the respective chromosome from cell1 and cell2 come together and mix. The mixing involves random parts of chromosome1 to get over-written by chromosome2. And the same in case of chromosome2. Hence, we are left with an inter mixed pair of chromosomes. Since chromosomes are carriers of genetic material, when there is mixing and intermingling of 2 chromosomes, there is also creation of new genetic sequences. Hence, this ensures genetic diversity. The formation of a more dominant / enhanced trait or a relatively sober trait can be attributed to such crossing over of chromosomes. Once crossover is through, the new offspring contains mixed characteristics for both the&amp;nbsp;parents.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Then what is mutation ?&lt;/em&gt; : In rare cases, when the crossing over causes some unexpected trait to be formed. This is termed as mutation. Statistically, mutation is very rare. A common example for mutation is cancer. Mutation can be either good or bad. Even evolution is a result of mutation. Single celled organisms have mutated to form multi-celled organisms and so on&amp;nbsp;&amp;#8230;. &lt;/p&gt;
&lt;p&gt;Once the simple process of cell reproduction is complete, we have a brand new offspring. The offspring contains traits of both the parents in random proportions. 
Similarly, many parents create multiple offspring, each offspring containing their traits. This set of offspring can be termed as a generation. We call it first generation. Again, when these offsprings (children) become parent , and produce their own offsprings, it gives rise to generation 2. Hence, when the process continues to go on, every generation can be said to have characteristics of the previous generation. Since the previous generation has characteristics of its&amp;#8217; previous generation, we can say that any randomly picked generation will have characteristics from all the previous&amp;nbsp;generations.&lt;/p&gt;
&lt;h1&gt;The &lt;span class="caps"&gt;CS&lt;/span&gt; Stuff&amp;nbsp;!!&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Now where do we apply all this biology ?&lt;/em&gt; Just as we have an offspring which is equivalent to a weighted addition of the characteristics of the two parents, similarly, a chessboard with a certain arrangement of queens can be said to be an offspring of 2 parent chessboards that may have similar arrangements. For example, suppose we have 2 parent chessboards (C1 and C2). And on reproduction, C1 and C2 produce the offspring chessboard &lt;span class="caps"&gt;OC&lt;/span&gt;. Based on the aforemention biological evidence, I can say&amp;nbsp;that &lt;/p&gt;
&lt;div class="math"&gt;$$ OC = f(C1, C2) $$&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\( f( )\)&lt;/span&gt; is simply a function that combines random characteristics of C1 with the remaining characteristics of&amp;nbsp;C2.&lt;/p&gt;
&lt;p&gt;So any chessboard arrangement of 8 queens can be computed as selections of certain arrangements from C1 and C2. Thereby, asserting the fact that I am creating &lt;span class="caps"&gt;OC&lt;/span&gt; through random probabilistic selection and not any pre-processed criteria. This is strictly following the model prescribed by nature. By keeping everything random, we are essentially replicating the random &amp;#8220;crossing over of genes&amp;#8221; that occurs in&amp;nbsp;cells.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Core&amp;nbsp;Working: &lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Initialization&lt;/em&gt; : The initialization contains of the randomly distributed population
that is generated. Lower population size will lead to lots of time in computation of
approximate solution. And higher value will cause internal iterations to increase.
Therefore choose population size carefully. In this example we have tried with
population size of 100 , 500 and&amp;nbsp;1000&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Selection of parents&lt;/em&gt;: Just as evolutionary biology requires two parents to undergo
meiosis, so does &lt;span class="caps"&gt;GA&lt;/span&gt;. Therefore there is need to select two parents which
determine a child. The calculation in determining the parents is elaborated&amp;nbsp;later&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CrossOver / Reproduction&lt;/em&gt; : Once parents having high fitness are selected,
crossover essentially marks the recombining of genetic materials / chromosomes
to produce a healthy&amp;nbsp;offspring&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Mutation&lt;/em&gt; : Mutation may or maynot occur. In case mutation occurs, it forces a
random value of child to change , thereby shifting the algorithm in either a
positive or negative&amp;nbsp;route&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Generation of new population&lt;/em&gt; : For all population, a child must be computed.
Therefore, the new population can be said to be a list dynamically populated by
the children&amp;nbsp;computed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;The&amp;nbsp;Algorithm:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Genetic-Algorithm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; population, Fitness-fn&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;individual&lt;/span&gt;
    &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;population&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;individuals&lt;/span&gt;
            &lt;span class="n"&gt;Fitness&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fn&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;measures&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;fitness&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;individual&lt;/span&gt;

    &lt;span class="n"&gt;repeat&lt;/span&gt;
        &lt;span class="n"&gt;parents&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;--&lt;/span&gt; &lt;span class="n"&gt;Select&lt;/span&gt; &lt;span class="n"&gt;parents&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;Population&lt;/span&gt;
        &lt;span class="n"&gt;population&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;--&lt;/span&gt; &lt;span class="n"&gt;Offsprings&lt;/span&gt; &lt;span class="n"&gt;created&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="n"&gt;parents&lt;/span&gt;

    &lt;span class="n"&gt;repeat&lt;/span&gt; &lt;span class="n"&gt;until&lt;/span&gt; &lt;span class="n"&gt;desired&lt;/span&gt; &lt;span class="n"&gt;fit&lt;/span&gt; &lt;span class="n"&gt;individual&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;obtained&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;individual&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;population&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Above is a simulated evolution algorithm that mimics production of offspring generation from a given population. The population refers to the possible inputs.The population can be defined as simply a collection of many individuals. So it simply means, all possible arrangements of the queens on a chessboard. 
Consider this. Just as every human being can be defined by the presence of characteristics, any solution to the 8 queens problem can be defined by the placement of the queens in non attacking arrangements. Hence, each parent is a chessboard having some arbitary arrangement of the queens. Moving on, every individual can be regarded as a strong offspring or a weak one. We measure stregth of a human offspring through physical strength, immunity, etc. Whereas in the case of 8 queens, the fitness of a board arrangement can be considered as the number of clashes that take place between the queens. So the measure of &lt;strong&gt;fitness of any individual (chessboard arrangement)&lt;/strong&gt; is attributed to &lt;strong&gt;number of clashes amongst attacking positions of queens&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitness fn : &lt;/strong&gt;
I said that the fitness fn is proportional to the number of clashes amongst the queens. If seen, there are 28 clashes possible in an 8 x 8 chessboard. Therefore, if an individual has high fitness, I can say that it will have lower number of&amp;nbsp;clashes.&lt;/p&gt;
&lt;div class="math"&gt;$$ \text{Let clashes} = \text{number of clashing queens}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ \therefore fitness = 28 - clashes $$&lt;/div&gt;
&lt;p&gt;Then, any individual with the maximum fitness will be having least number of&amp;nbsp;clashes.&lt;/p&gt;
&lt;h1&gt;Implementation&amp;nbsp;:&lt;/h1&gt;
&lt;p&gt;You can checkout my implementation &lt;a href="https://gist.github.com/kushalvyas/7f777c24880c8d1dee744ecb8125e50f"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Setting up population &lt;/strong&gt; : Your population is eventually going to determine the output. Each individual of the population is a board arrangement. So, firstly let&amp;#8217;s describe the&amp;nbsp;individual&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;BoardPosition&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;Attributes&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Sequence&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Fitness&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Survival&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/ac3593026e0a431bbc47aaf4e025fe67.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;def generateChromosome():
	# randomly generates a sequence of board states.
	global nQueens
	init_distribution = np.arange(nQueens)
	np.random.shuffle(init_distribution)
	return init_distribution

def generatePopulation(population_size = 100):
	global POPULATION

	POPULATION = population_size

	population = [BoardPosition() for i in range(population_size)]
	for i in range(population_size):
		population[i].setSequence(generateChromosome())
		population[i].setFitness(fitness(population[i].sequence))

	return population
&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;p&gt;As seen, each board position contains a sequence : the arrangement of queens on the board. A fitness value ( as discussed above) and a survival value. We need a survival value simply because evolutionary primitives say so. According to Darwin&amp;#8217;s theory, &amp;#8216;Survival of the fittest&amp;#8217;, each parent (board position) has to be able to survive. This survival influences the ability of the parent to&amp;nbsp;reproduce.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Determining&amp;nbsp;Fitness &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Secondly, we define a fitness function. As discussed above, we shall characterize fitness as a maximum value. Even though less number of clashes mean higher fitness , for implementation purposes, we consider maximum values of fitness. This can be done becuase we subtract number of clashes from&amp;nbsp;28.&lt;/p&gt;
&lt;p&gt;To determine clashes, each queen has to be checked&amp;nbsp;for &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Row&amp;nbsp;clashes&lt;/li&gt;
&lt;li&gt;Column&amp;nbsp;clashes&lt;/li&gt;
&lt;li&gt;Diagonal&amp;nbsp;clashes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The ideal case can yield upton 28 arrangements of non attacking&amp;nbsp;pairs.&lt;/p&gt;
&lt;p&gt;Therefore max fitness = &lt;span class="math"&gt;\(&amp;nbsp;28.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence fitness val returned will be &lt;span class="math"&gt;\( 28 - \text{&amp;lt;number of&amp;nbsp;clashes&amp;gt;}\)&lt;/span&gt; &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;clashes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c"&gt;# calculate row and column clashes&lt;/span&gt;
&lt;span class="c"&gt;# just subtract the unique length of array from total length of array&lt;/span&gt;
&lt;span class="c"&gt;# [1,1,1,2,2,2] - [1,2] =&amp;gt; 4 clashes&lt;/span&gt;
&lt;span class="n"&gt;row_col_clashes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;clashes&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;row_col_clashes&lt;/span&gt;

&lt;span class="c"&gt;# calculate diagonal clashes&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;dy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;chromosome&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dy&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;clashes&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;


&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;clashes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt; Determining Survival of&amp;nbsp;individual &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having demostrated how to calculate the fitness value, we can now move on to defining a survival function. A survival function will determine as to how able is a particular parent to survive and produce a healthy offspring. If out of 1000 people, the probability of any random person getting picked is &lt;span class="math"&gt;\( \frac{1}{1000}\)&lt;/span&gt;, similarly, the survival function can be written as a normalized probability of a individual getting selected based upon the fitness&amp;nbsp;:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$ \text{Survival for} i^{th} \text{individual} = \frac{f_i}{\sum_{j=1}^{j=populationSize} f_j } $$&lt;/div&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Selection of Parents  &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;&amp;nbsp;Crossover&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Parent selection is quite straightforward. Simply selecting 2 random - non equal parent will do the trick, but is very inaccurate and time costing. Hence, by using the survival function, elimination of weaker parent can be performed. Eventually, the parents left will be ones that are more likely to produce a fit offspring. Analogous to biology, two dominant traits will more likely produce a dominant trait cause it won&amp;#8217;t matter which of the dominant trait is&amp;nbsp;selected. &lt;/p&gt;
&lt;p&gt;Once two parents are selected, the proceeding part is crossover. Crossover involves selecting a part of parent 1 and concatenating it with parent 2. Why &amp;#8230; Because our chromosomes do so. This selective addition of chessboard board sequences will create an offspring that may have one of these three&amp;nbsp;characteristics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It may either be a solution to the 8 queens problem. In that case, save the output and discard those parents &amp;#8212;&amp;gt; child&amp;nbsp;path. &lt;/li&gt;
&lt;li&gt;It may have a higher fitness than either parent. This will also help in its&amp;nbsp;survival&lt;/li&gt;
&lt;li&gt;It may have a lower fitness value, thereby lowering its&amp;nbsp;survival&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any of the three cases is still capable of producing a next generation. The only difference will be regarding the probability of that offspring to yield a substantially fit&amp;nbsp;offspring.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mutation : &lt;/strong&gt;
Biologically and statistically, mutation is continously occuring at very low frequencies. In human genetic theory, mutation involves alteration within the structure of the chromosome, within the structure of the gene. To replicate this for 8 queens problem, one can simply alter a board&amp;nbsp;arrangement. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="fitness_cros" src="http://kushalvyas.github.io/images/fitness_cros.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Source : Peter norvig genetic algorithm illustration&lt;/em&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The above illustrates the entire flow from initial population to fitness
function calculation, to selection to cross-over and finally mutation. &lt;strong&gt;Note : Mutation may or may not provide with the&amp;nbsp;solution.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;How to interpret input and output&amp;nbsp;sequences:&lt;/h1&gt;
&lt;p&gt;Each row of the chess row is indexed from &lt;span class="math"&gt;\(0 \text{-&amp;gt;} 7\)&lt;/span&gt; . The sequence &lt;span class="math"&gt;\([\text{ a b c d  .... }]\)&lt;/span&gt; means that in &lt;span class="math"&gt;\(0^{\text{th}}\)&lt;/span&gt; column, &lt;span class="math"&gt;\(a^{\text{th}}\)&lt;/span&gt; row,  the queen is present and so on. The following are example sequences that are (not) a solution to the&amp;nbsp;problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\([\text{1 1 1 1 1 1 1 1}]\)&lt;/span&gt; : This is certainly not a solution. All queens lie in the &lt;span class="math"&gt;\(1^{\text{st}}\)&lt;/span&gt; row of the board. Hence all are&amp;nbsp;conflicting&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\([\text{1 1 0 4 2 1 2 4}]\)&lt;/span&gt; : Similarly, thi aint a solution&amp;nbsp;either.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\([\text{1 6 2 5 7 4 0 3}]\)&lt;/span&gt; : This satisfies 8 queen problem and hence a&amp;nbsp;solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Output and termination criteria&amp;nbsp;:&lt;/h1&gt;
&lt;p&gt;Well, it seems that such an algorithm will keep producing offsprings and again their offsprings. When to stop. In my implementation , I&amp;#8217;ve kept a limit of iteration. This is just for purposes of conviniece. However, at anytime, the most optimal solution can be described as the &lt;strong&gt;most fit board position&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
    &lt;img alt="im" src="http://kushalvyas.github.io/images/op8q.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You can check out the &lt;a href="https://gist.github.com/kushalvyas/7f777c24880c8d1dee744ecb8125e50f"&gt;code&lt;/a&gt; at &lt;a href="https://www.github.com/kushalvyas/"&gt;Github&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="AI"></category><category term="Py"></category></entry><entry><title>Bag of Visual Words Model for Image Classification andÂ Recognition</title><link href="http://kushalvyas.github.io/BOV.html" rel="alternate"></link><updated>2016-07-13T20:40:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-07-13:BOV.html</id><summary type="html">&lt;p&gt;Bag of Visual Words is an extention to the &lt;span class="caps"&gt;NLP&lt;/span&gt; algorithm Bag of Words used for image classification. Other than &lt;span class="caps"&gt;CNN&lt;/span&gt;, it is quite widely used. I sure want to tell that &lt;span class="caps"&gt;BOVW&lt;/span&gt; is one of the finest things I&amp;#8217;ve encountered in my vision explorations until&amp;nbsp;now.&lt;/p&gt;
&lt;p&gt;So what&amp;#8217;s the difference between Object Detection and Objet Recognition .. !! 
Well, recognition simply involves stating whether an image contains a specific object or no. whereas detection also demands the position of the object inside the image. So say, there is an input image containing a cup, saucer, bottle, etc. The task is to be able to recognize which of the objects are contained in the&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;BOV&lt;/span&gt; was developed by &lt;a href="" target="_blank"&gt;CSurka et. al&lt;/a&gt; essentially creates a vocabulary that can best describe the image in terms of extrapolable features. It follows 4 simple&amp;nbsp;steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determination of Image features of a given&amp;nbsp;label &lt;/li&gt;
&lt;li&gt;Construction of visual vocabulary by clustering, followed by frequency&amp;nbsp;analysis&lt;/li&gt;
&lt;li&gt;Classification of images based on vocabulary&amp;nbsp;genereated&lt;/li&gt;
&lt;li&gt;Obtain most optimum class for query&amp;nbsp;image &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To implement this, we shall be using : Opencv (3.x), sklearn (0.17), caltech101 dataset( trimmed&amp;nbsp;version)&lt;/p&gt;
&lt;p&gt;Lets first understand what a feature is. One can say that a feauture is any discernable, and a significant point/group of points in an image. What to select as a feature depends on the application such as corner points, edges, blobs, &lt;span class="caps"&gt;DOG&lt;/span&gt; , etc. And to ease out our troubles, &lt;a href="http://www.cs.ubc.ca/~lowe/keypoints/" target="_blank"&gt;David Lowe&lt;/a&gt; developed &lt;a href="http://www.cs.ubc.ca/~lowe/keypoints/" target="_blank"&gt;&lt;span class="caps"&gt;SIFT&lt;/span&gt;&lt;/a&gt; : Scale Invariant Feature Transform. &lt;span class="caps"&gt;SIFT&lt;/span&gt; is extensively used today. We will be using &lt;span class="caps"&gt;SIFT&lt;/span&gt; as well. Please note, that algorithms such as &lt;span class="caps"&gt;SIFT&lt;/span&gt;, &lt;span class="caps"&gt;SURF&lt;/span&gt; which are patented are not available in the master version of Opencv-Itseez. To be able to use it, either install opencv_contrib or &lt;span class="caps"&gt;VLFEAT&lt;/span&gt; (You may want to check out my &lt;a href="http://kushalvyas.github.io/setup_env.html"&gt;previous post on environment settings &lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Lets begin with a few introductory concepts required Bag of words. We shall cover 4 parts (so keep scrolling&amp;nbsp;!)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Bag of Visual Words&amp;nbsp;Model&lt;/li&gt;
&lt;li&gt;Generating&amp;nbsp;Vocabulary&lt;/li&gt;
&lt;li&gt;Training and&amp;nbsp;testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt; : Lets say there is a bunch of Wrigleys Skittles. And someone is to tell you to group them according to their color. It&amp;#8217;s quite simple .. aint it! Simply seperate all red, blue, green, etc in different parts of the room. Here, we differentiated and seperated them on basis of &lt;strong&gt;color only&lt;/strong&gt;.&lt;br /&gt;
So moving on to a more complex situation that would give a much profound meaning to clustering. Suppose there is a room full of utilities, be it accesories, clothing, utensils, electronics, etc. Now, if someone is told to seperate out into well formed groups of similar items, one would essentially be performing&amp;nbsp;clustering. &lt;/p&gt;
&lt;p&gt;So yes, clustering can be said as the grouping a set of objects in such a way that objects in the same group are much similar, than to those in other&amp;nbsp;groups/sets&lt;/p&gt;
&lt;p&gt;Moving on, lets&amp;#8217; decide as to how we perform clustering. The selection of clustering algorithm depends more on what kind of similarity model is to be chosen. There are cases wherein, the plain&amp;#8217;ol clustering impression that everyone so simply elucidates may not be the right choice. For example, there exists various models, such as centroid oriented - Kmeans, or Distribution based models - that involve clustering for statistical data; such places require Density based clustering (&lt;span class="caps"&gt;DBSCAN&lt;/span&gt;) ,&amp;nbsp;etc. &lt;/p&gt;
&lt;p&gt;Beginning with &lt;strong&gt;KMeans clustering&lt;/strong&gt;. Suppose there are X objects, that are to be divided into K clusters. The input can be a set of features, &lt;span class="math"&gt;\(X = \{ x_1, x_2, ..., x_n \}\)&lt;/span&gt;. The goal is basically to minimize the distance between each point in the scatter cloud and the assigned&amp;nbsp;centroids. &lt;/p&gt;
&lt;div class="math"&gt;$$ {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}
 $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;   is mean of points for each &lt;span class="math"&gt;\(S_i\)&lt;/span&gt;(cluster) and &lt;span class="math"&gt;\(S\)&lt;/span&gt; denotes set of points partitioned into clusters of &lt;span class="math"&gt;\(\{ S_1, S_2, ... S_i&amp;nbsp;\}\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;Hence it can be said that for each cluster centroid, there exists a group of point around it, known as the&amp;nbsp;center. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We first define an initial random solution. This initial solution can be called as the cluster centroids. They need to be randomly placed within the bounds of data , and not so callously&amp;nbsp;random. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second comes the &lt;strong&gt;Assignment Step&lt;/strong&gt;. What happens here, is that KMeans iterates over each of the input feature / datapoint and decides which is the closest cluster centroid w.r.t itself. Once the closest centroid is established, it is then alloted to that particular&amp;nbsp;centroid.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next is the &lt;strong&gt;Average &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Update Step&lt;/strong&gt;. Once we are able to perform the first, most crude clustering, we shall relocate the cluster centroids. The newly computed cluster centroids can be said to be the aggregate of all members of that particular cluster. Hence the centroid moves more inwards for a tightly alligned distribution and more outwards&amp;nbsp;otherwise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once the avergaing step is accomplished, and the new clusters are computed, the same process is repeated over and over again. Untill &amp;#8230;&amp;nbsp;!!! &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ideal condition for stoppage is when there is no change in position of the newly computed cluster centroid with respect to its previous position. This can be further interpreted as that the distance of every datapoint inside its cluster will be minimum w.r.t its mean i.e its centroid. However, there can be a minimum threshold value to stop the clustering process from going on and&amp;nbsp;on.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Okay, below is a snippet showing how to use KMeans clustering algorithm as provided in &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;scikit&lt;/a&gt;.&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/590b7e1adad10656793cfb78f041193a.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;
n_samples = 1000
n_features = 5;
n_clusters = 3;

# aint this sweet 
X, y = make_blobs(n_samples, n_features) 
# X =&gt; array of shape [nsamples,nfeatures] ;;; y =&gt; array of shape[nsamples]

# X : generated samples, y : integer labels for cluster membership of each sample
# performing KMeans clustering

ret =  KMeans(n_clusters = n_clusters).fit_predict(X)&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;!-- output --&gt;

&lt;p&gt;&lt;img alt="clusteroutput" src="http://kushalvyas.github.io/images/clusterplot1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Here&amp;#8217;s a sample output for clustering using 3 clusters on a set of 1000 samples.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why are we using Clusterng &amp;#8230; Why Kmeans &amp;#8230;: &lt;/strong&gt;
KMeans performs clustering. It is one of the widely used algorithms when it comes to unsupervised learning. Bag of visual words uses a training regimen that involves, firstly, to partition similar features that are extrapolated from the training set of images. To make it more easily understandable, think of it this way. Every image has certain discernable features, patterns with which humans decide as to what the object perceived is. When you see a image of &amp;#8230; umm. let&amp;#8217;s say a motorbike -  significant features are being extrapolated. This features together help in deciding whether what is being seen is actually a motorbike. The collection as well as frequency of particular features is what helps in estimating what object does the image&amp;nbsp;contain,&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Some Prerequisites :&lt;/u&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First of all, you need a training set. If you&amp;#8217;re using a personal computer, I&amp;#8217;d recommend to use a truncated version of any publically available image datasets (if you&amp;#8217;re worried about your &lt;span class="caps"&gt;PC&lt;/span&gt; taking up too much time during training) or perhaps train using a minimally bounded set of features. I&amp;#8217;d recommend using &lt;a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/"&gt;Caltech101&lt;/a&gt;. Also check out Caltech256, &lt;span class="caps"&gt;CIFAR10&lt;/span&gt; datasets. They&amp;#8217;re good !! As in real&amp;nbsp;good. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Secondly please set up either &lt;span class="caps"&gt;LIBSVM&lt;/span&gt; , &lt;span class="caps"&gt;SKLEARN&lt;/span&gt;,  &lt;span class="caps"&gt;VLFEAT&lt;/span&gt; ( for enhanced vision algos&amp;#8230; like sift) Library, or Any python machine learning toolkit that will provide basic &lt;span class="caps"&gt;SVM&lt;/span&gt; , Kmeans functionaliy. You can visit my previous post on &lt;a href="http://kushalvyas.github.io/setup_env.html"&gt;setting up environments&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Third, please maintain a descent project directory structure. Well documented and well assembled as to where input , output and logs will be stored. This will help a lot for further projects and especially when it comes to making your code&amp;nbsp;modular.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So lets&amp;#8217;&amp;nbsp;proceed. &lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;Bag of Visual&amp;nbsp;Words&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;This is a supervised learning model. There will be a training set and a testing set.
You can find my implementation on &lt;a href="https://github.com/kushalvyas/Bag-of-Visual-Words-Python" target="_blank"&gt;Github&lt;/a&gt;. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the downloaded dataset into training and testing. You can use the 70-30 ratio or 80-20. But keep in mind, if the training data is not good , there &lt;span class="caps"&gt;WILL&lt;/span&gt; be discrepencies in the&amp;nbsp;output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="caps"&gt;BOVW&lt;/span&gt; is an example of supervised learning. It&amp;#8217;s always better to keep a mapping of which images belong to what classification label ( a label can be defined as a key/value for identifying to what class/category does the object&amp;nbsp;belongs).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract features from the training image sets. One can use opencv_contrib/ vl feat  for &lt;a href="{filename}/articles/features.md"&gt;Feature Extration&lt;/a&gt;(&lt;span class="caps"&gt;SIFT&lt;/span&gt;, &lt;span class="caps"&gt;SURF&lt;/span&gt; more popularly). This essentially converts the image into a feature&amp;nbsp;vector. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The final step is codebook generation. A codebook can be thought of as a dictionary that registers corresponding mappings between features and their definition in the object. We need to define set of words (essentially the features marked by words) that provides an analogous relation of an object ( being trained) w.r.t. a set of&amp;nbsp;features. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- 
[gist:id=4e5044cc533096f45323de43b85000be]
 --&gt;

&lt;p&gt;Project Architecture&amp;nbsp;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="nb"&gt;dir&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
            &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
                &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;obj1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
                &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;obj2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;

            &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
                &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;obj1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
                &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;obj2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;

    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;helpers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
    &lt;span class="o"&gt;|-&lt;/span&gt; &lt;span class="n"&gt;Bag&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;


&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;Bag&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;train_set&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;span class="math"&gt;\(images\)&lt;/span&gt; directory contains testing and training images. Provide the path to test and train images to &lt;span class="math"&gt;\(Bag.py\)&lt;/span&gt;&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;We shall go through each module, step by step. &lt;code&gt;Bag.py&lt;/code&gt; contains the main. We have the methods &lt;code&gt;trainModel&lt;/code&gt; and &lt;code&gt;testModel&lt;/code&gt;. &lt;code&gt;Heplers.py&lt;/code&gt; contains various helper functionalities. It contains Imagehelpers, FileHelper, BOVhelpers. Imagehelpers contains colorscheme conversion, feature detection. FileHelper returns a dictionary of each object-name with a corresponding list of all images. It also returns total image count. (required&amp;nbsp;later)&lt;/p&gt;
&lt;p&gt;FileHelper will return the training set. It returns a dictionary with &lt;code&gt;key = object_name&lt;/code&gt; and &lt;code&gt;value = list of images&lt;/code&gt; and total number of&amp;nbsp;images.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;FileHelper&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;getFiles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    - returns  a dictionary of all files &lt;/span&gt;
&lt;span class="sd"&gt;    having key =&amp;gt; value as  objectname =&amp;gt; image path&lt;/span&gt;
&lt;span class="sd"&gt;    - returns total number of files.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;imlist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; #### Reading image category &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; ##### &amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;imlist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;imagefile&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Reading file &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imagefile&lt;/span&gt;
            &lt;span class="n"&gt;im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;imagefile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;imlist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;imlist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;ImageHelpers&amp;#8217;s primary function is to provide with &lt;span class="caps"&gt;SIFT&lt;/span&gt; features present in an image. We require these image features to develop our vocabulary.(I&amp;#8217;ll explain what it means in the coming parts ..&amp;nbsp;). &lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ImageHelpers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sift_object&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xfeatures2d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SIFT_create&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cvtColor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gray&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;keypoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;descriptors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sift_object&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detectAndCompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;keypoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;descriptors&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;I will reiterate the algorithm once again, and now step by step. With the input image list, firstly compute features. You&amp;#8217;ll need objects of all helper classes. So please initialize them in your main module. Having detected and computed &lt;span class="caps"&gt;SIFT&lt;/span&gt; features, we need to process a&amp;nbsp;vocabulary.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How to develop visual vocabulary&amp;nbsp;?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;A picture is worth a thousand words &amp;#8230;&amp;nbsp;&amp;#8220;&lt;/p&gt;
&lt;p&gt;Suppose I say, there is an image of a shoe. Humans tend to describe using normal english language words. A person will probably descibe a shoe as a shoe! A bit elaborated version may be , something having laces and small netted structures. Go a bit further, and it&amp;#8217;ll seem to have small holes/circles, a few curved lines, a bunch of very striking corner points, a few patches having high contrast. Now you&amp;#8217;re speaking in terms of a vision. One can dumb down the idea of descriptors / features in an image as striking/significant portions of the image that help describe it. Every image contains multiple features i.e. if we were to mathematically express an image in terms of features, we would say that an Image is a collection of features , where every feature may have a certain frequency of&amp;nbsp;occurence.&lt;/p&gt;
&lt;p&gt;If it is so, how can we differentiate w.r.t features. !!! The answer comes from a much natural origin. Suppose a human was asked to differentiate between a shoe and a lipstick :P . He/she would start describing each of the aforementioned item. Shoe would have small circular pathes, long laces, much curved portions, etc. On the other hand, a lipstick is quite cylindrical and has a top buldge. So i can say , 
&lt;br&gt;&lt;/br&gt;
&lt;strong&gt; for a set of given features, there exists a weighted combination of features that describe an image individually &lt;/strong&gt;
&lt;br&gt;&lt;/br&gt;
and that &lt;br&gt;&lt;/br&gt;
&lt;strong&gt;Every feature present in an image, can be used as means for describing the same image&lt;/strong&gt;
&lt;br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;From the above two statements, we define what a visual word is. Simply any thing that can be used to describe an image , we consider them as a visual word. Thus, our image becomes a combination of visual words (that are essentially features). And to make it more mathematical, we define this structure as a histogram. Essentially, histogram is just a measure of frequency occurence of a particular item, here in our case, we will be describing each image as a histogram of features. How many features out of the total vocabulary are required to make sense of what the computer is looking at&amp;nbsp;. &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Linking vocabulary and clustering &lt;/strong&gt;:&lt;/h3&gt;
&lt;p&gt;Using &lt;span class="caps"&gt;SIFT&lt;/span&gt;, we detect and compute features inside each image. &lt;span class="caps"&gt;SIFT&lt;/span&gt; returns us a &lt;span class="math"&gt;\(m \times 128\)&lt;/span&gt; dimension array, where m is the number of features extrapolated. Similarly, for multiple images, say 1000 images, we shall&amp;nbsp;obtain &lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{bmatrix}
features_0\\ 
features_1\\ 
    ....      \\ 
    ....       \\
    ....       \\
features_{n}\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ where \  features_i \ \ is \ \ a  \ \ array \  of  \ \ dimension \ \  m \times 128 $$&lt;/div&gt;
&lt;p&gt;We now have a full stacked up list of what visual words are being used for every image. The next task is to group similar features. Think of it as synonyms &amp;#8230;. Similar features can provide an approximate estimate as to what the image is, just as synonyms tend to express upon the gist of a sentence. Therefore when the machine is trained over several images, similar features that are able to describe similar portions of the image are grouped together to develop a vast vocabulary base. Each of these group collectively represent the a word and all groups in totality yields us the complete vocabulary generated from the training data. Hence there is a screaming need for clustering in the said&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;If we were to allot a definition to any of the similar words, we can simply refer them by their cluster&amp;nbsp;number.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="image" size:small="size:small" src="http://kushalvyas.github.io/images/grid.png" /&gt;
&lt;!-- &lt;img alt="image" src="http://kushalvyas.github.io/images/vocab.png" /&gt; &lt;/center&gt; --&gt;&lt;/p&gt;
&lt;p&gt;A more illustrative visualization of the histogram can be observed in the adjacent&amp;nbsp;figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" size:small="size:small" src="http://kushalvyas.github.io/images/vocab_unnormalized.png" /&gt;&lt;/p&gt;
&lt;p&gt;The above image shows how a collective vocabulary will look like. Encomprising of the total number of each type of feature/word present in the training set in&amp;nbsp;totality.&lt;/p&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s some implementation snippet as to how one would implement&amp;nbsp;this&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/ad29abbc3b68b1cf530490bbe4eaec73.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;def developVocabulary(self,n_images, descriptor_list, kmeans_ret = None):
		
		"""
		Each cluster denotes a particular visual word 
		Every image can be represeted as a combination of multiple 
		visual words. The best method is to generate a sparse histogram
		that contains the frequency of occurence of each visual word 

		Thus the vocabulary comprises of a set of histograms of encompassing
		all descriptions for all images

		"""

		self.mega_histogram = np.array([np.zeros(self.n_clusters) for i in range(n_images)])
		old_count = 0
		for i in range(n_images):
			l = len(descriptor_list[i])
			for j in range(l):
				if kmeans_ret is None:
					idx = self.kmeans_ret[old_count+j]
				else:
					idx = kmeans_ret[old_count+j]
				self.mega_histogram[i][idx] += 1
			old_count += l
		print "Vocabulary Histogram Generated"
&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;p&gt;As seen, the input is n_images i.e. the total number of images and descriptor_list, that contains the feature descriptor array ( one discussed above, the full stacked up list of features). Our histogram is therefore of the size &lt;span class="math"&gt;\(n\_images \times n\_clusters\)&lt;/span&gt; thereby defining each image in terms of generated vocabulary. During the definition phase, we need to locate the cluster that contains the features i.e the cluster number whose cluster centroid is nearest to the location of the current&amp;nbsp;feature. &lt;/p&gt;
&lt;p&gt;This completes the most important part of the vocabulary generation. Now time to train the&amp;nbsp;machine.&lt;/p&gt;
&lt;h3&gt;&lt;u&gt; Training the machine to understand the images using &lt;span class="caps"&gt;SVM&lt;/span&gt;   &lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;Our &lt;span class="math"&gt;\(mega\_histrogram\)&lt;/span&gt; is basically an array of size &lt;span class="math"&gt;\(n\_samples, n\_significantFeatures\)&lt;/span&gt;. Meaning, the number of rows, that are defined as the &lt;span class="math"&gt;\(n\_imagses\)&lt;/span&gt; in the above snippet, are nothing but samples we need to train. Each row contains a distribution/combination of visual words used to describe the image. All we need is a multiclass classifier to distinguish between similar images and to define classes for the&amp;nbsp;same.&lt;/p&gt;
&lt;p&gt;for classification purposes, I&amp;#8217;d recommend to use sklearn. Its by far the most easily adaptable &lt;span class="caps"&gt;API&lt;/span&gt; i&amp;#8217;ve used. And goes very well with numpy&amp;nbsp;datastructures. &lt;/p&gt;
&lt;p&gt;Just so you know, Coding up an &lt;span class="caps"&gt;SVM&lt;/span&gt; on your own is a herculean task, hence we follow a 1-2-3 methodology as projected by &lt;span class="caps"&gt;SKLEARN&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c"&gt;# make classifier object&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# train the model&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;#returns a list of prediction for each test_data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That&amp;#8217;s it !! Once trained, the model is ready for testing&amp;nbsp;!! &lt;/p&gt;
&lt;h3&gt;Implementation&amp;nbsp;Details&lt;/h3&gt;
&lt;p&gt;Please maintain the aforementioned project dir structure and follow naming conventions. You may want to read the repositories readme for further details. Here&amp;#8217;s what predictions I obtained on testing and training the model against a trimmed version of Caltech101&amp;nbsp;dataset. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;p&gt;There are a few snaps of outputs when the model was tested on the limited version of the dataset&amp;nbsp;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="im1" src="http://kushalvyas.github.io/images/ac1.png" /&gt;
&lt;img alt="im2" src="http://kushalvyas.github.io/images/bk1.png" /&gt;
&lt;img alt="im3" src="http://kushalvyas.github.io/images/bk2.png" /&gt;
&lt;img alt="im4" src="http://kushalvyas.github.io/images/dollar1.png" /&gt;
&lt;img alt="im5" src="http://kushalvyas.github.io/images/sbb3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;Checking for&amp;nbsp;Accuracy&lt;/h3&gt;
&lt;p&gt;Accuracy measure is one of the most important steps in &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms .A Confusion matrix is basically how many test cases were correctly classified. Hence, a confusion matrix is used to determine the accuracy of classification. On having tried it on the limited dataset, below is the &lt;span class="caps"&gt;CF&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;
    &lt;img alt="imc" src="http://kushalvyas.github.io/images/normalized_7.png" /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;So now you know how to write your own Image Classifiers and Recognizers&amp;#8230; !! Hurrayy !! There are tremendous application when it comes to intelligence and computer vision. Especially in this field. If you wanna check for accuracy measures in classification, be sure to implement a &lt;a href=""&gt;Confusion Matrix&lt;/a&gt; .Meanwhile, use the &lt;a href="http://kushalvyas.github.io/BOV.html"&gt;Bag of Visual Words&lt;/a&gt; and create some cool&amp;nbsp;stuff&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Check out the &lt;a href="https://github.com/kushalvyas/Bag-of-Visual-Words-Python"&gt;Code&lt;/a&gt; on &lt;a href="https://www.github.com/kushalvyas/"&gt;Github&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="ObjectRecognition"></category></entry><entry><title>Simple Object RecognitionÂ techniques</title><link href="http://kushalvyas.github.io/Obj_CV.html" rel="alternate"></link><updated>2016-07-13T00:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-07-13:Obj_CV.html</id><summary type="html">&lt;p&gt;Object recognition is one of the hot topics in computer vision. Particularly, of much significance, it has various applications in robotics, identification, interpretation and such image oriented&amp;nbsp;tasks. &lt;/p&gt;
&lt;p&gt;One can ramify between Object recognition into 4 major subsets such as&amp;nbsp;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Template based approach&lt;/strong&gt; : We take the object and match the object with distribution pattern of intensities. It&amp;#8217;s quite exhaustive is not invariant to geometric transformations inside search image
    Eg: Template Matching . However, this isint much&amp;nbsp;sturdy.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Geometric approach&lt;/strong&gt; : Recognize the object on basis of edges, corners, angles. Then we create a one 2  one match and then make a transformation that handles rotation, scaling, etx.&lt;br /&gt;
    Eg: Matching w.r.t positioning of objects. Classifying&amp;nbsp;patterns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph based approach&lt;/strong&gt; : we match similar types of structures. Encode the relationship between structures rather than the geometry of the&amp;nbsp;figure.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bag of Visual Words&lt;/strong&gt; : I will cover Bag of Words in a seperate post &lt;a href="http://kushalvyas.github.io/BOV.html"&gt;Bag of Words&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;I&amp;#8217;ll be going through a couple of common techniques. Firstly I&amp;#8217;ll introduce template matching and in the follow, using probabilistic recognition , then moving on to Machine learning applications such as using &lt;span class="caps"&gt;BOW&lt;/span&gt; models coupled with &lt;span class="caps"&gt;SVM&lt;/span&gt; , or feature extrapolation and recognition using neural&amp;nbsp;nets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Template based aproach&lt;/strong&gt; 
Well, there are  a few contentions i;d like to make when we use this approach. Shape based approach is not the key or the best method for recognition. But , in industry, all methods have their perks. Say we have an industry converyor belt that is suppossed to carry only boxes or T- Joints or anything so simply extrapolable and uniform that it is feasible to use shape matching rather than going for complex approaches like using nets, or using &lt;span class="caps"&gt;SVM&lt;/span&gt; along with bag of words. Then I&amp;#8217;d say, template matching or shape based matching is the best bet in terms of accuracy as well as&amp;nbsp;speed. &lt;/p&gt;
&lt;p&gt;Lets dive a little deeper in the aforementioned topic.
I&amp;#8217;ll be using template matching as an example to explain how Shape based matching works. Consider two images. The template image (T) and the actual image (I). We say that we want to find the location / occurence of template inside the actual&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;In simple words I can say, return True, iff &lt;span class="math"&gt;\(T \subseteq I\)&lt;/span&gt;. But life aint so simple. In reality, every pixel needs to be iterated over and checked. It can be said that the a template is belonging to a particular image iff, all pixels match exactly. i.e. the sum of ( Absolute Difference between every pixel must be&amp;nbsp;zero).&lt;/p&gt;
&lt;div class="math"&gt;$$SAD(x, y) = \sum_{i=0}^{T_{\text{rows}}}\sum_{j=0}^{T_{\text{cols}}} {\text{Diff}(x+i, y+j,i,j)}$$&lt;/div&gt;
&lt;p&gt;Here&amp;#8217;s how the above formula&amp;nbsp;works.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(SAD\)&lt;/span&gt; : sum of absolute differences.
&lt;span class="math"&gt;\(SAD (x, y)\)&lt;/span&gt; : the &lt;span class="caps"&gt;SAD&lt;/span&gt; of a particular pixel location in the actual Image &lt;span class="math"&gt;\(I\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;Each pixel in &lt;span class="math"&gt;\(I\)&lt;/span&gt; can be represented using &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt;. It has an intensity, or color value expresed as &lt;span class="math"&gt;\(Intensity(I(x,y))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, a pixel in template image &lt;span class="math"&gt;\(T(p,q)\)&lt;/span&gt; will have intensity as &lt;span class="math"&gt;\(Intensity(T(p,q))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A search window is created that is of the size of template image. The window is then slid over the actual Image I and every time &lt;span class="caps"&gt;SAD&lt;/span&gt; is computed. For checking single occurence of template the iterations can be stopped once &lt;span class="caps"&gt;SAD&lt;/span&gt; is &lt;span class="math"&gt;\(0\)&lt;/span&gt;, or else it can be continued until the end of &lt;span class="math"&gt;\(I\)&lt;/span&gt; is&amp;nbsp;reached. &lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll be implementing the algorithm using opencv ,&amp;nbsp;python &lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/c151fd726bb3d439b721b264f3d50843.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;"""
Template matching using opencv

Run : python file.py &lt;templateImage&gt; &lt;actualImage&gt;


"""

import sys, cv2
import numpy as np

def main(template, actualImage):
	tImage = cv2.imread(template, 0)
	aImage = cv2.imread(actualImage)
	aCopy = aImage.copy()
	aImage = cv2.cvtColor(aImage, cv2.COLOR_BGR2GRAY)
	th, tw = tImage.shape[:2]

	match = cv2.matchTemplate(aImage, tImage, cv2.TM_SQDIFF)
	minmaxlocs = cv2.minMaxLoc(match)
	topcorner = minmaxlocs[-2]
	bottomcorner=  (topcorner[0] + tw, topcorner[1]+th)
	cv2.rectangle(aCopy, topcorner, bottomcorner, (0, 0, 255), 2)

	cv2.imshow("actual Iamge", aCopy)
	cv2.waitKey()


if __name__ == '__main__':
	args = sys.argv[1:]
	print args
	if (len(args) is not 2):
		print "Please pass cmd args for template and actual image"
		sys.exit(-1)

	main(args[0], args[1])&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;p&gt;Take a look at &lt;span class="math"&gt;\(line 14\)&lt;/span&gt;, it states &lt;code&gt;cv2.TM_SQDIFF&lt;/code&gt; parameter in matchTemplate(). Opencv&amp;#8217;s rendition for &lt;span class="caps"&gt;SAD&lt;/span&gt; can be delineated as&amp;nbsp;: 
&lt;/p&gt;
&lt;div class="math"&gt;$$R(x, y) = \sum_{i, j}\ {(\text{T}(i,j) - \text{I}(x+i, y+j))^2}$$&lt;/div&gt;
&lt;p&gt;And here is the output ! We will search a pair of glares from this paraphenalia
&lt;center&gt;
&lt;img alt="crop" src="http://kushalvyas.github.io/images/tempCrop.jpg" /&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="original" src="http://kushalvyas.github.io/images/optemplate.png" /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Okay, so template matching can be established using the opencv api. Its&amp;#8217; quite simple to implement in plain ol c++ as well. Not a biggie.  We&amp;#8217;ll use the helper functions for image reading , accessing pixel value, etc. but the rest can be quite easily implemented using a bunch of &lt;code&gt;for loops&lt;/code&gt;&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/0667c6a249f3397ee7de1db0cf04555d.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;/*
Author : Kushal Vyas
Code for template matching as implemented by opencv.

*/
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include "iostream"
#include "cmath"

using namespace std;
using namespace cv;

int main(){
	cout&lt;&lt;"Template matching \n";
	Mat tImage, aImage, aCopy;
	tImage = imread("tempCrop.jpg",0);
	aImage = imread("temA1.jpg",0);
	aCopy = imread("temA1.jpg");


	// performing template matching using SAD method
	Size Tsize, Isize ;
	Tsize = tImage.size();
	Isize = aImage.size();

	
	long int minSAD = 999999999999, SAD=0;
	int xloc=0, yloc=0, sadloc=0;
	// loop through the actual image
	int a_i, a_j;
	for (a_i =0; a_i &lt; Isize.height - Tsize.height; a_i++){
		for (a_j=0; a_j &lt; Isize.width - Tsize.width; a_j++){
			SAD = 0;

			// check every pixel of actual image with the template image
			// loop through the template image
			for(int t_j=0; t_j &lt; Tsize.width; t_j++){
				for(int t_i=0; t_i &lt; Tsize.height; t_i++){
					uchar aT = aImage.at&lt;uchar&gt;( a_i+t_i, a_j+t_j );
					uchar tT = tImage.at&lt;uchar&gt;(t_i, t_j);
					SAD += pow(abs(int(aT) - int(tT)),2);
				}
			}
			// cout &lt;&lt;"SASD sis "&lt;&lt;SAD&lt;&lt;endl; 
			// save the best found position
			if(minSAD &gt; SAD ){
				cout &lt;&lt;" SAD "&lt;&lt; SAD&lt;&lt;endl;
				minSAD = SAD;
				yloc = a_i;
				xloc = a_j;
				sadloc = SAD;
			}
		}
	}

	cout&lt;&lt;"Template size"&lt;&lt;Tsize&lt;&lt;endl;
	cout&lt;&lt;"Image size"&lt;&lt;Isize&lt;&lt;endl;

	cout&lt;&lt;a_i&lt;&lt;" "&lt;&lt;a_j&lt;&lt;endl;

	cout&lt;&lt;xloc&lt;&lt;","&lt;&lt;yloc&lt;&lt;" SAD is "&lt;&lt;sadloc&lt;&lt;endl;

	// create a rectangle to demarkate the region
	rectangle(aCopy, Point(xloc, yloc), Point(xloc+Tsize.width, yloc+Tsize.height), Scalar(0,0,255), 2);

	// imshow("Template image", tImage);
	imshow("Actual image", aImage);
	imshow("Matched Image", aCopy);
	waitKey();
	destroyAllWindows();

	return 0;
}&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;You can find my remaining code on &lt;a href="https://github.com/kushalvyas/ObjectDetectionPython"&gt;Github&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; remaining posts on other approaches coming soon&amp;nbsp;&amp;#8230; &lt;/strong&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="ObjectRecognition"></category><category term="CV"></category></entry><entry><title>Setup Computer VisionÂ Environment</title><link href="http://kushalvyas.github.io/setup_env.html" rel="alternate"></link><updated>2016-06-20T00:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-06-20:setup_env.html</id><summary type="html">&lt;p&gt;Well, If this is a blog on some of the cool stuff in Computer vision, I think i must mention about the api&amp;#8217;s that one can use, or how to setup your&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;So yes, there is a myraid choice available to start developing. I personally like &lt;a href="http://opencv.org/"&gt;OpenCV&lt;/a&gt; because of its&amp;#8217; enormous community support along with its multiplatform&amp;nbsp;facilities.&lt;/p&gt;
&lt;p&gt;Apart from this, there is &lt;a href="http://in.mathworks.com/products/matlab/"&gt;&lt;span class="caps"&gt;MATLAB&lt;/span&gt;&lt;/a&gt;. 
Whoaaa&amp;#8230; this is like a computing powerhouse. With so many easy to use functionality, and 
simply clicking your way into outputs, &lt;span class="caps"&gt;MATLAB&lt;/span&gt; has one hell of a&amp;nbsp;pricetag.&lt;/p&gt;
&lt;p&gt;Apart from these, there are&amp;nbsp;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SimpleCV&lt;/strong&gt; : This is actually computer vision made easy.There is no need to spend endless time in figuring out data structures, Matrices, what color format to select ( &lt;span class="caps"&gt;8UC1&lt;/span&gt;, &lt;span class="caps"&gt;32FC1&lt;/span&gt;,etc)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VxL Libraries&lt;/strong&gt; (I like them too.. I&amp;#8217;ll use them the day I make a robot :P&amp;nbsp;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLFeat&lt;/strong&gt; : This may come out very handy !! It contains some of the finest implementations in vision as well as image understanding&amp;nbsp;algorithms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageJ&lt;/strong&gt; : A java library for primarily image&amp;nbsp;processing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="caps"&gt;CCV&lt;/span&gt;&lt;/strong&gt; ( Haven&amp;#8217;t used it yet, so .. what do i know&amp;nbsp;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Anyway, moving on to the installation, there are plenty of online resources that can be used for downloading and setting up&amp;nbsp;OpenCV.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For compiling opencv c++&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;span class="caps"&gt;CMAKE&lt;/span&gt;. It&amp;#8217;s much simpler to compile using&amp;nbsp;cmake&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can use this boiler plate file if incase&amp;nbsp;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Filename : CMakeLists.txt&lt;/span&gt;
&lt;span class="n"&gt;cmake_minimum_required&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VERSION&lt;/span&gt; &lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;project_name&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;find_package&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;OpenCV&lt;/span&gt; &lt;span class="n"&gt;REQUIRED&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;add_executable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;project_name&lt;/span&gt; &lt;span class="n"&gt;project_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpp&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;target_link_libraries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;project_name&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;OpenCV_LIBS&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;simply execute&amp;nbsp;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cmake&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;make&lt;/span&gt; 
&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;project_name&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Incase you&amp;#8217;re using &lt;strong&gt;python&lt;/strong&gt;, &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;To be&amp;nbsp;continued&lt;/strong&gt;&lt;/p&gt;</summary><category term="CV"></category></entry><entry><title>WikiÂ Lookup</title><link href="http://kushalvyas.github.io/wiki_lookup.html" rel="alternate"></link><updated>2016-04-12T00:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-04-12:wiki_lookup.html</id><summary type="html">&lt;p&gt;Pin :&amp;nbsp;true&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;A Chrome Extension for simplifying Wikipedia Surfing.Wiki Lookup features the use of dynamically generated popups to simplify your Wikipedia Surfing Experience.Just hover over the hyperlink, and wait for a small dialog to appear.No Need to open up new tabs and feel lost while surfing wikipedia.org. Its&amp;#8217; light-weight and&amp;nbsp;compact.&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;img alt="img_normal" src="http://kushalvyas.github.io/images/Menu_026.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Everytime you hover over a wiki link, it&amp;#8217;ll show an appropriate amount of text to define the&amp;nbsp;term.&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;img alt="img_normal" src="http://kushalvyas.github.io/images/img_ext1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Install the Extension&amp;nbsp;:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Clone the &lt;a href="https://github.com/kushalvyas/Wiki-Look-Up"&gt;repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Please use google chrome with developer mode&amp;nbsp;enabled&lt;/li&gt;
&lt;li&gt;Goto&amp;nbsp;chrome://extensions&lt;/li&gt;
&lt;li&gt;Download the .crx extensions file. And simply drag and drop it to the extensions&amp;nbsp;page.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;OR&lt;/span&gt; Load unpacked Extension ( provide path to&amp;nbsp;repository/wiki_look_up/)&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;OR&lt;/span&gt; &lt;a href="https://chrome.google.com/webstore/detail/wiki-lookup/lkejjmneaehckeodpaknhndcnaokdokc?utm_source=chrome-ntp-icon"&gt;Get it here @ Chrome Web&amp;nbsp;Store&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Enjoy surfing wiki&amp;nbsp;:P&lt;/p&gt;</summary><category term="dev"></category><category term="technical"></category><category term="wikipedia"></category></entry><entry><title>Using the Samba Server and SSH</title><link href="http://kushalvyas.github.io/Samba%20SSH.html" rel="alternate"></link><updated>2016-04-07T15:55:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-04-07:Samba SSH.html</id><summary type="html">&lt;h3&gt;The Samba Server and &lt;span class="caps"&gt;SSH&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Well,this article was long due. Speaking from the perspective of a newcomer to linux, the samba server was something that had caught my eye in my sophomore year. That very next day I had setup a small system at my home. The blog that you are reading right now is stored in my home server. I don&amp;#8217;t have any files locally on my laptop.&amp;nbsp;B) &lt;/p&gt;
&lt;p&gt;Yes.. Speaking of &lt;a href="https://www.samba.org/"&gt;Samba&lt;/a&gt;, its quite easy to setup and install it. You can refer the steps &lt;a href="https://help.ubuntu.com/community/How%20to%20Create%20a%20Network%20Share%20Via%20Samba%20Via%20CLI%20(Command-line%20interface/Linux%20Terminal)%20-%20Uncomplicated,%20Simple%20and%20Brief%20Way!"&gt;here&lt;/a&gt; but ya.. i&amp;#8217;ll be going over them&amp;nbsp;too..&lt;/p&gt;
&lt;p&gt;For, ubuntu, I&amp;#8217;d say, simply run these&amp;nbsp;commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;apt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;update&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;apt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;samba&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once you have installed samba, open the &lt;code&gt;/etc/samba/smb.conf&lt;/code&gt; file and make the changes as&amp;nbsp;below.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a folder to be shared on the network ( /home/&lt;username&gt;/&lt;name_of_folder&gt; )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next,    &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;cp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;samba&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;smb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here you are essentially copying and placing the samba conf file in a safe&amp;nbsp;location.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;nano&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;vi&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;gedit&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;basically&lt;/span&gt; &lt;span class="n"&gt;anytexteditor&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;samba&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;smb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the very end of the file . (If your using nano , simply press &amp;#8216;&lt;span class="caps"&gt;ALT&lt;/span&gt;&amp;nbsp;/&amp;#8217; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following&amp;nbsp;snippet&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;name_of_folder&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;username&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;/&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;name_of_folder&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;read&lt;/span&gt; &lt;span class="n"&gt;only&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Save the file and restart&amp;nbsp;samba &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="n"&gt;smbd&lt;/span&gt; &lt;span class="n"&gt;restart&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To access any samba file, install smbclient. You can run an apt-get install&amp;nbsp;smbclient&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;To checkin &lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;smbclient&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;ipaddress&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;foldername&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Some Tips&lt;/strong&gt;
If you plan on using this, i recommend that you make the server ip static. Goto your router settings and reserve the ip for whichever system you are putting this on. It&amp;#8217;s much simpler to do&amp;nbsp;so.&lt;/p&gt;
&lt;p&gt;Next I also recommend you to set up an &lt;span class="caps"&gt;SSH&lt;/span&gt; connection over your local machines. This is what I use. Using samba for convinient file transfers and &lt;span class="caps"&gt;SSH&lt;/span&gt; to control further&amp;nbsp;commands.&lt;/p&gt;
&lt;p&gt;Setting up &lt;span class="caps"&gt;SSH&lt;/span&gt; for local networks is very simple. Install &lt;span class="caps"&gt;SSH&lt;/span&gt; using&amp;nbsp;`&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;apt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;SSH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and simply&amp;nbsp;type&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ssh&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="nd"&gt;@IP&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Make sure , both the endpoint machines are equipped with the&amp;nbsp;dependencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Just fire it up and enjoy&amp;#8230; Access your files freely and sit back on your bean bags and&amp;nbsp;relax&lt;/strong&gt;&lt;/p&gt;</summary></entry><entry><title>Python Graphs and TopologicalÂ Sort</title><link href="http://kushalvyas.github.io/graph_py.html" rel="alternate"></link><updated>2016-04-05T10:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-04-05:graph_py.html</id><summary type="html">&lt;h3&gt;Graphs using&amp;nbsp;python&lt;/h3&gt;
&lt;p&gt;It was about yesterday that I was working on a project that involved the use of a graphs and topological sort. However, I was surprised when I saw the implementation sizes. Albeit , python uses substantially less number of &lt;span class="caps"&gt;LOC&lt;/span&gt;, this example particularly blew my mind. Ive never been able to create graphs in such an easy manner. :P :P (Im overstating cause im a bit ecstatic right now)&amp;nbsp;.&lt;/p&gt;
&lt;p&gt;Beginning with graphs, We&amp;#8217;ll represent the graph using lists, and not matrices (saves memory you&amp;nbsp;know).. &lt;/p&gt;
&lt;p&gt;The graph refers the one shown here (Courtesy : geeksforgeeks.com)&lt;center&gt;
    &lt;img alt="graphimage" src="http://d1gjlxt8vb0knt.cloudfront.net//wp-content/uploads/graph.png" /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s write the python code. We shall read the file, and then create the adjacency lists out of&amp;nbsp;it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# number of vertices&lt;/span&gt;
&lt;span class="n"&gt;vertices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;

&lt;span class="c"&gt;#create adj lists&lt;/span&gt;
&lt;span class="n"&gt;adj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c"&gt;#create a function to set an edges&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertex_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node_no&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;globals&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;adj&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vertex_no&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node_no&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# print adj&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s write a toposort now. A topological sort will give the order as to which node is to be processed first in order of execution. It has to start with a node having in degree as 0. You can &lt;a href="https://en.wikipedia.org/wiki/Topological_sorting"&gt;read here &lt;/a&gt; for more&amp;nbsp;details&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;vertices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;adj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;visited&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertex_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node_no&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;globals&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;adj&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vertex_no&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node_no&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;globals&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;visited&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;adj&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visited&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;visited&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;#print &amp;quot;i = &amp;quot;, i&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;#print output&lt;/span&gt;


&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;set_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# print adj&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;visited&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;


&lt;span class="sb"&gt;`OUTPUT`&lt;/span&gt;
&lt;span class="sb"&gt;`[5, 4, 2, 3, 1, 0]`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you want to make customised nodes, with some attributes, 
you can create a&amp;nbsp;class.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;initialize&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;node_no&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;


&lt;span class="n"&gt;Then&lt;/span&gt; &lt;span class="n"&gt;append&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="n"&gt;into&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;adj&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary></entry><entry><title>An introduction to ComputerÂ Vision</title><link href="http://kushalvyas.github.io/CV.html" rel="alternate"></link><updated>2016-04-04T08:00:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-04-04:CV.html</id><summary type="html">&lt;h3&gt;Computer&amp;nbsp;Vision&lt;/h3&gt;
&lt;p&gt;Computer vision involves image processing, a bit of &lt;span class="caps"&gt;AI&lt;/span&gt; and various algorithms. Basically every gesture effect that you see today,  or the &lt;a href="http://www.digitalspy.com/tech/news/a476446/intel-magic-mirror-allows-shoppers-to-try-on-clothes-virtually/"&gt;Intel Magic Mirror&lt;/a&gt; all dumbs down to a few concepts of &lt;span class="caps"&gt;CV&lt;/span&gt;. One can say , that &lt;span class="caps"&gt;CV&lt;/span&gt; is the next big thing that&amp;#8217;s there today&amp;nbsp;&amp;#8230; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;But there&amp;#8217;s this one great hell of a snag .. that is &lt;strong&gt;Illumination&lt;/strong&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;However, there are methods to overcome that&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;In this blog, I&amp;#8217;ll be discussing various Image Processing and Computer vision fundamentals and posting small implementable&amp;nbsp;snippets.&lt;/p&gt;</summary><category term="CV"></category></entry><entry><title>'The Social Network' where it allÂ started</title><link href="http://kushalvyas.github.io/utils.html" rel="alternate"></link><updated>2016-03-24T10:20:00+05:30</updated><author><name>Kushal Vyas</name></author><id>tag:kushalvyas.github.io,2016-03-24:utils.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer : &lt;/em&gt;If ur a windows user &amp;#8230; forgive&amp;nbsp;me&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving on, i think every person starting off with linux, must know a few things ..
Specially, if you,ve just watched &lt;em&gt;&lt;a href="http://www.imdb.com/title/tt1285016/"&gt;The Social Network&lt;/a&gt;&lt;/em&gt; and you&amp;#8217;ve seen  Jesse Eisenberg hacking away at his&amp;nbsp;screen&amp;#8230; &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="gif" src="http://kushalvyas.github.io/images/mkz.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;You&amp;#8217;re probably wondering what is &lt;em&gt;wget&lt;/em&gt;, &lt;em&gt;emacs&lt;/em&gt;, &lt;em&gt;apache&lt;/em&gt; and much more. If not, please rewatch the movie with&amp;nbsp;subtitiles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wget&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/software/wget/" target="_blank"&gt;Wget&lt;/a&gt; is a basic downloading utility, actually calling it basic would be an understatement
You can download files, directories, recursive download, filter through files and download them, ftp, multiple downloads, get selected elements and crawl through pages&amp;#8230;It&amp;#8217;s an amazing&amp;nbsp;utility. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you don&amp;#8217;t have wget, simple run an apt-get wget &lt;br&gt;&lt;code&gt;wget http://www.example.com&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or if you want to get all files from the browser directory, &lt;br&gt; &lt;code&gt;wget -r --no-parent http://your_directory_url&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;or if you go all &lt;span class="caps"&gt;SPARTA&lt;/span&gt; on the website, &lt;br&gt;&lt;code&gt;wget 
     --recursive 
     --no-clobber 
     --page-requisites 
     --html-extension 
     --convert-links 
     --restrict-file-names=windows 
     --domains url_domain 
     --no-parent 
         url&lt;/code&gt;
more details, check out this &lt;a href="http://www.linuxjournal.com/content/downloading-entire-web-site-wget" target="_blank"&gt;linux journal post&lt;/a&gt; or head out to &lt;a href="https://www.gnu.org/manual/manual.html" target="_blank"&gt;Wget&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go through the docs .. find whatever suits your&amp;nbsp;need&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Emacs &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;span class="caps"&gt;VI&lt;/span&gt;&lt;/strong&gt;&amp;nbsp;: &lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Editor_war"&gt;Emacs or &lt;span class="caps"&gt;VI&lt;/span&gt;&lt;/a&gt; , you&amp;#8217;d better end up choosing one and not telling the other side. People will go to war with you, if you say emacs is better than &lt;span class="caps"&gt;VI&lt;/span&gt; or vice versa .
&lt;center&gt;&lt;img alt="evn" src="http://kalyanvarma.net/images/struggle1.gif" /&gt;&lt;/p&gt;
&lt;p&gt;And the my favourite .. click to play
&lt;center&gt;&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/R0Kzgno5wxA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;!-- 
&lt;a href="https://www.youtube.com/watch?v=R0Kzgno5wxA" title="Should I Learn Emacs or Vi?"&gt;klzzwxh:0016&lt;/a&gt;{:target='_blank'} --&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Learning either is useful. These are highly customizable editors. You will absolutely stop using other tools once u get accustomed to either of these. Read along &lt;a href="http://unix.stackexchange.com/questions/986/what-are-the-pros-and-cons-of-vim-and-emacs" target="_blank"&gt;this &lt;span class="caps"&gt;SO&lt;/span&gt; post&lt;/a&gt; to uncover&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;GREP&lt;/span&gt;&amp;nbsp;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;GREP&lt;/span&gt;, is by far my favourite utility. An amazing tools to perform searches. It uses a regular expression pattern matcher, so it&amp;#8217;s quite easy to detect&amp;nbsp;substrings.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using grep for cli searches:&lt;br&gt;&lt;code&gt;ls --all | grep filname&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using grep with wget to cli search a webpage&lt;br&gt;&lt;code&gt;wget -O - url | grep keyword&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;&lt;a href="http://kushalvyas.github.io/images/blog_grep.png"&gt;&lt;img alt="grep" src="http://kushalvyas.github.io/images/blog_grep.png" /&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I can go on and on about the utilities, but now I think, it&amp;#8217;s time to check them 
out&amp;nbsp;yourself.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last thing, before I sign out&amp;nbsp;.. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C-Matrix&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, pretending to be a hacker, like in those &lt;span class="caps"&gt;MATRIX&lt;/span&gt; movies, is pretty cool.. and probably, you can pick up someone (let&amp;#8217;s assume that , that&amp;#8217;s&amp;nbsp;possible)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo apt-get install cmatrix&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and this is the beauty of it
&lt;center&gt;
&lt;img alt="cmatrix" src="http://kushalvyas.github.io/images/cmatrix.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Enjoy , Browse, &amp;#34; Make the world a better place&amp;#34;(Courtesy : &lt;a href="https://www.youtube.com/watch?v=69V__a49xtw"&gt;Silicon Valley&lt;/a&gt; )&amp;nbsp;:P  &lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;</summary><category term="utils"></category></entry></feed>