<!doctype html>
<html class="no-js" lang="en">
<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="http://kushalvyas.github.io/theme/css/materialize.min.css">
        <!-- <link rel="stylesheet" type="text/css" href=""> -->
        <script src="http://kushalvyas.github.io/theme/js/jquery-1.11.3.min.js"></script>
        <script src="http://kushalvyas.github.io/theme/js/materialize.min.js"></script>
        <link rel="stylesheet" href="http://kushalvyas.github.io/theme/css/foundation.min.css" media="all">
        <script type="text/javascript" src="http://kushalvyas.github.io/theme/js/modernizr.js"></script>
        <link href='https://fonts.googleapis.com/css?family=Exo+2:400,300,700,300italic,400italic,700italic,900,900italic' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" type="text/css" href="http://kushalvyas.github.io/theme/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="http://kushalvyas.github.io/theme/css/pygments.min.css">
        <link rel="stylesheet" type="text/css" href="http://kushalvyas.github.io/theme/css/subscribe.css">
        <!-- <script src="http://kushalvyas.github.io/theme/syntax/scripts/shAutoloader.js" type="text/javascript"></script> -->
        <!-- <link href="http://kushalvyas.github.io/theme/syntax/styles/shCore.css" rel="stylesheet" type="text/css" /> -->
        <!-- <link href="http://kushalvyas.github.io/theme/syntax/styles/shThemeRDark.css" rel="stylesheet" type="text/css" /> -->
        <!-- <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"> -->
        <link rel="stylesheet" href="http://kushalvyas.github.io/theme/css/lamboz.css" media="all">
        <title>Caffe + ConvNets : Visual Recognition Made&nbsp;Easy - BitsMakeMeCrazy <br> Kushal Vyas's Blog</title>
        <meta charset="utf-8" />
        <link href="http://kushalvyas.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="BitsMakeMeCrazy <br> Kushal Vyas's Blog Full Atom Feed" />
        <link href="http://kushalvyas.github.io/feeds/object-recognition-classification-cv.atom.xml" type="application/atom+xml" rel="alternate" title="BitsMakeMeCrazy <br> Kushal Vyas's Blog Categories Atom Feed" />
  

        <script src="//load.sumome.com/" data-sumo-site-id="cecf6965e36602158241ee45155d77c8c02f27706e5178b61597811ab112b3a8" async="async"></script>   
        <style type="text/css">
        /*deactivate materialize ul li effecr*/
            ul li {
                list-style: browser-default;
            }
            li { list-style: circle; }
        </style>
       
</head>
<body>
    <div class="pages">
        <ul>
            <li class="home"><a href="http://kushalvyas.github.io/index.html">Home</a></li>
            <li ><a href="http://kushalvyas.github.io/pages/about-me.html">About&nbsp;me</a></li>
            <li ><a href="http://kushalvyas.github.io/pages/technical.html">Computer&nbsp;Vision</a></li>
            <li ><a href="http://kushalvyas.github.io/pages/projects.html">Projects</a></li>
            <li ><a href="http://kushalvyas.github.io/pages/Music.html">Music</a></li>

            <li style="float :right;" class=""><a href="http://www.github.com/kushalvyas" target="_blank"><i class="fa fa-github fa-2x" aria-hidden='true'></i>&nbsp;&nbsp;</a></li>
            <li style="float :right;" class=""><a href="mailto:kushalvyaskv@gmail.com" target="_blank"><i class="fa fa-envelope-o fa-2x" aria-hidden='true'></i>&nbsp;&nbsp;</a></li>
            <li style="float :right;" class=""><a href="https://twitter.com/kushal_kv" target="_blank"><i class="fa fa-twitter fa-2x" aria-hidden='true'></i>&nbsp;&nbsp;</a></li>
            <li style="float :right;" class=""><a href="https://www.youtube.com/channel/UCoAFKWmi3Wca8hj5GSEriRQ" target="_blank"><i class="fa fa-youtube fa-2x" aria-hidden='true'></i>&nbsp;&nbsp;</a></li>
            <li style="float :right;" class=""><a href="https://in.linkedin.com/in/kushal-vyas-44529ba9" target="_blank"><i class="fa fa-linkedin fa-2x" aria-hidden='true'></i>&nbsp;&nbsp;</a></li>
        </ul>
    </div>
<div class="hp-header-inner">
    <div class="page-header">
        <div class="content-header">
            <div id="title-block">
            <h1>  <a href="http://kushalvyas.github.io/caffe_cnn.html" rel="bookmark"
                    title="Permalink to Caffe + ConvNets : Visual Recognition MadeÂ Easy">Caffe + ConvNets : Visual Recognition Made&nbsp;Easy</a></h1>
            </div>
        </div>
    </div>
</div>
    <div class="content">
        <div class="data-holder">
        <div class="row">
           <div class="large-10 content-column column">
<div class="content-article">
    <header class="header-article">
        <div class="read-more">Posted on Fri 20 January 2017</div>
 
   </header>
    <section class='article'>
        
        <div class="entry-summary">
            <p>Train and test Convolutional Nets on a dataset using&nbsp;Caffe</p>
        </div>
    
        <div class="entry-content">
            <p>To classify, recognize and localize objects in an image is a hot topic in Computer Vision and  throughout the years various models have been established for the same. My previous post on <a href="http://kushalvyas.github.io/BOV.html" target="_blank">Bag of Visual Words Model for Image Classification and Recognition</a> illustrates one such model. Today I write about <a href="http://cs231n.github.io/convolutional-networks/" target="_blank">Convolutional Neural Networks</a> and how to implement them in <a href="http://caffe.berkeleyvision.org" target="_blank">Caffe</a>. and how to train a <span class="caps">CNN</span> for your own dataset. We will be using the standard dataset, but you can organize any other/personal dataset in a similar&nbsp;fashion.</p>
<h2>Basics of <span class="caps">CNN</span>&nbsp;:</h2>
<p>A typical Convolutional Neural Net or <strong><span class="caps">CNN</span></strong>, is a feed-forward neural net, with the input being an image. Its&#8217; major objective it to establish a hierarchy of spatial features present in an image using which it is able to classify. The architecture of the net comprises of convolutional layers, pooling layers , activation layer, and fully connected&nbsp;layers.</p>
<p>One can refer <a href="http://cs231n.github.io/convolutional-networks/" target="_blank">this post by Andrej Karpathy</a> to understand the intricate details of a&nbsp;ConvNet.</p>
<p><center><img alt="typical_conv" src="http://kushalvyas.github.io/images/cnn_images/typical_cnn.png" /></center>
<center>Source : <a href="https://en. wikipedia.org/wiki/File:Typical_cnn.png" target="_blank">Convolutional Neural Nets : Wiki</a></center></p>
<p><strong>Convolution Layer : <span class="caps">CONV</span></strong> </p>
<p>The primary operator is a convolution operation. For a 1-D signal it is expressed&nbsp;as 
</p>
<div class="math">$$ y(n) = x(n) * h(n) = \sum_{k = - \infty}^{k = + \infty}x[k] . h[n-k]$$</div>
<p>However, an image is a 2D signal (stored as a 2D matrix). To convolve an image, we use a convolution kernel, which is simply a 2D&nbsp;matrix.</p>
<div class="math">$$ convolve(I_1, I_2) = I_1[x, y] * I_2[x, y] = \sum_{n_1= - \infty}^{ n_1 = + \infty} \text{   } \sum_{n_2 = - \infty}^{ n_2 = + \infty} I_1[n_1, n_2].I_2[x - n_1, y - n_2]$$</div>
<p>Here&#8217;s an example of how it&nbsp;works</p>
<p><img alt="cn2d" src="http://kushalvyas.github.io/images/cnn_images/con2d.png" /></p>
<p>For this 7 x 7 matrix, and a kernel of 3 x 3, the kernel will slide over the matrix one column, at a time. Once it reaches the end of the matrix (columnwise), it&#8217;ll shift to the next row. At every position, the dot product is&nbsp;computed. </p>
<p>If the kernel is overlapped with the (0 , 0) position , we find the dot product&nbsp;of </p>
<div class="math">$$ 
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\ 
3 &amp; 2 &amp; 2 \\ 
2 &amp; 4 &amp; 3
\end{bmatrix} \text{ * }
\begin{bmatrix} 
-1 &amp; -1 &amp; -1 \\ 
-1 &amp; 8 &amp; -1 \\ 
-1 &amp; -1 &amp;  -1 
\end{bmatrix} = \text{-1}$$</div>
<p>which essentially is&nbsp;: </p>
<p><span class="math">\(1\times-1 + 1\times-1 + 1\times-1 + 3\times-1 + 2\times8 + 2\times-1 + 2\times-1 + 4\times-1 + 3\times-1 =&nbsp;-1\)</span></p>
<p>Similarly, assume an input image - <span class="math">\(I\)</span>  is of Size <span class="math">\((200 \text{ x } 200)\)</span>, and we have a kernel - <span class="math">\(k\)</span> of size <span class="math">\((10 \text{ x } 10)\)</span>. The convolution of this kernel over this image is to basically, take the kernel and slide it over the image column by column, row by row. Initially the kernel is placed at <span class="math">\(I[0, 0]\)</span>. Therefore the kernel covers a region of <span class="math">\(10 \text{ x } 10\)</span> (size of kernel) starting from <span class="math">\(I[0, 0] \text{ to } I[10,10]\)</span>. Once the dot product (convolution) is computed over this pixel region, the kernel is shifted by one column to the right. Now the kernel occupies an image region of <span class="math">\(I[0,1 ] \text{ to } I[10, 11]\)</span> and so on. 
<center><img alt="conv_img" src="http://kushalvyas.github.io/images/cnn_images/convolution.png" /></center></p>
<p><center>Source: <a href="http://stackoverflow.com/questions/15356153/how-do-convolution-matrices-work" target="_blank">Stackoverflow : How convolution matrices work</a></center></p>
<p>A series of convolution operations take place at every layer, which extrapolates the pertinent information from the images. At every layer, multiple convolution operations take place, followed by zero padding and eventually passed through an activation layer and what is outputted is an Activation&nbsp;Map. </p>
<p><strong>Filters and&nbsp;Stride</strong></p>
<p>An image can be represented as a 3D array. The dimension being <span class="math">\(rows, cols, depth\)</span>. The 
&#8216;depth&#8217; refers to the channels of the image, namely <span class="math">\(\text{red, green and blue}\)</span>. Hence, a color <span class="caps">RGB</span> image of size <span class="math">\(200 \text{ x } 200\)</span> is actually of size <span class="math">\(200 \text{ x } 200 \text{ x } 3\)</span>. To convolve this image, we need a kernel that not only convolves spatially, i.e along the rows and columns but also reaches out to the values in all the channels. Hence, the kernel used will be of a size <span class="math">\(k = 10 \text{ x } 10 \text{ x } 3\)</span>. <strong>Why 3?</strong> Because this will convolve through the depth of the image. The filter depth must be same as the depth of the input from the previous layer. Each filter can be moved over the image, column by column and row by row -&gt; this means that the stride is 1. Inshort, The number of pixels skipped whilst the filter traverses the image is stride. One can have a filter with stride &#8216;s&#8217;, implying that it&#8217;ll skip &#8216;s&#8217; rows/columns while moving towards the other end of the image, convolving at each&nbsp;step.</p>
<!-- <center>![conv_layer]({filename}/images/cnn_images/conv_layer.png)</center>
<center>Source : [Convolutional Neural Nets : Wiki](https://en.wikipedia.org/wiki/File:Conv_layer.png)</center>

 -->

<p><strong>Activation and&nbsp;Pooling</strong></p>
<p>An activation function basically defines the output of the neuron for the given input. Take a simple Hebbian Learning example . An activation function will return <span class="math">\(1\)</span> if input is <span class="math">\( &gt; 0\)</span> and return <span class="math">\(0\)</span> otherwise. The value of input being greater that 0, is nothing but a threshold value. Instead of 0, any other value can also act as a threshold. In ConvNets, a the popular activation function used is a Rectifier or <span class="caps">RELU</span>. </p>
<div class="math">$$f(x) = \text{max(0, x)}$$</div>
<p>This means that the output of the convolution is thresholded at 0. All positive values are returned as is and all negative values are made&nbsp;0. </p>
<p><center><img alt="relu_plot" src="http://kushalvyas.github.io/images/cnn_images/relu.png" /></center></p>
<p>For our above example, if the convolution output is passed to the Rectifier unit, it&#8217;ll set all negative values to&nbsp;zero. </p>
<p><img alt="rell image" src="http://kushalvyas.github.io/images/cnn_images/rell.png" /></p>
<p>On the other hand, the pooling operation is used to down-sample / reduce the activation map. This essentially reduces the volume of the middle-stages output. The output size is computed the same way as computed in the <span class="caps">CONV</span> layer. An important thing to note is that is that during the convolution layer, there is a rapid reduction in the dimensionality of the input. To maintain it at a constant size through multiple <span class="caps">CONV</span> layers,zero padding is used. Zero padding will pad the image with zeros on its boundaries making it of the same size as the input. It is in the pooling layer where the size reduction takes&nbsp;place.</p>
<h2>Implementation with&nbsp;Caffe</h2>
<p><strong><a href="http://caffe.berkeleyvision.org/" target="_blank">Caffe</a></strong> is a framework for deep learning, very popular for its simplicity in implementing <span class="caps">CNN</span>&#8217;s. It has been developed by the Berkely Vision <span class="amp">&amp;</span> Learning Center. You can use a <span class="caps">CPU</span> as well as a <span class="caps">GPU</span> mode. It is widely known that when it comes to matrix and other such image operations , most of which can be done in parallel, <span class="caps">GPU</span>&#8217;s triumph over <span class="caps">CPU</span>&#8217;s. I used my <span class="caps">CPU</span> machine to train a limited Caltech101 dataset, and it went on for 3&nbsp;days. </p>
<p><strong>Setting&nbsp;Up</strong></p>
<p>The example covers a classification tutorial with Caffe and your own dataset. Before starting off, it is important that Caffe and the following modules are&nbsp;setup.</p>
<p><em><a href="https://www.vision.caltech.edu/Image_Datasets/Caltech101/" target="_blank">Caltech101 limted dataset</a></em> : This comprises of 101 object categories which can be used to test and learn classification. You can download and extract the dataset in the working&nbsp;directory. </p>
<p><em><a href="https://www.dropbox.com/s/y3k1tnlhymo2xd8/caltechNET_train_iter_356.caffemodel?dl=0" target="_blank">My Pre-Trained Caltech101 Model</a></em> : Incase you are low on computing power, or would just like to test the code, you can simply download my pretrained&nbsp;network.</p>
<p><strong>Preparing <span class="caps">LMDB</span> format&nbsp;data</strong></p>
<p>This article will now cover how to make a custom dataset , and train it using caffe. There are constraints, wherein caffe uses an lmdb data format, and it is important to convert your dataset into the respective&nbsp;formats.</p>
<p>Next is converting the images into an lmdb format. The caffe documentation mentions to generate a txt file comprising of the image path with its associated class number. You can write a simple python script listing contents of your training and testing directories into such text&nbsp;files</p>
<div class="highlight"><pre><span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">sunflower</span><span class="o">/</span><span class="n">image_0020</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">0</span>
<span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">Motorbike</span><span class="o">/</span><span class="n">image_0033</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">1</span>
<span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">soccer_ball</span><span class="o">/</span><span class="n">image_0042</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">4</span>
<span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">accordion</span><span class="o">/</span><span class="n">image_0258</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">6</span>
<span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">dollar_bill</span><span class="o">/</span><span class="n">image_0673</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">2</span>
<span class="o">&lt;</span><span class="n">path_to_caltech101</span><span class="o">&gt;/</span><span class="n">limited</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">airplanes</span><span class="o">/</span><span class="n">image_0050</span><span class="o">.</span><span class="n">jpg</span> <span class="mi">3</span>
</pre></div>


<p><br>
Once done, use the Caffe <code>convert_imageset</code> to convert these images into the leveldb/lmdb data format. The default backend option is an lmdb backend. The <code>convert_imageset</code>  can be found at <code>caffe/build/tools</code>. </p>
<div class="highlight"><pre><span class="p">.</span><span class="o">/</span><span class="n">convert_imageset</span> <span class="o">--</span><span class="n">resize_height</span><span class="o">=</span><span class="mi">200</span> <span class="o">--</span><span class="n">resize_width</span><span class="o">=</span><span class="mi">200</span> <span class="o">--</span><span class="n">shuffle</span> <span class="err">$</span><span class="n">PATH_TO_IMAGESET</span> <span class="err">$</span><span class="n">PATH_TO_TEXT_FILE_CRAEATED_ABOVE</span><span class="p">.</span><span class="n">txt</span> <span class="err">$</span><span class="n">OUTPUT_LMDB_LOCATION</span>
</pre></div>


<p>Or simply change the paths and location inside the <code>caffe/examples/imagenet/create_imagnet.sh</code> file</p>
<p><br>
Eventually your working directory will comprise of a train <span class="caps">LMDB</span> directory and a test <span class="caps">LMDB</span>&nbsp;directory.    </p>
<div class="highlight"><pre>    <span class="o">|-</span> <span class="n">src</span>
    <span class="o">|-</span> <span class="n">train_lmdb_folder</span>
    <span class="o">|-</span> <span class="n">test_lmdb_folder</span>
</pre></div>


<p><br><br></p>
<p><strong>Defining the Network Architecture</strong>:</p>
<p>Caffe network architectures are very simple to define. Create a file &#8220;model.prototxt&#8221; and define the network architecture as follows. We will be using the AlexNET model (winner of ImageNet challenge 2012).
Given below is a representation of how the net looks&nbsp;like.</p>
<p><img alt="net_arch" src="http://kushalvyas.github.io/images/cnn_images/net_arch.png" /></p>
<p><a href="http://kushalvyas.github.io/images/cnn_images/net_arch.png" target="_blank">Click here and zoom to view&nbsp;it </a></p>
<p>To define the architecture, open the model.prototxt file. The model begins with a net name&nbsp;: </p>
<div class="highlight"><pre>        <span class="n">name</span> <span class="o">:</span> <span class="s">&quot;NameoftheNET&quot;</span>
</pre></div>


<p>Then sequentially start defining layers. 
First comes the data layer. Any layer that needs to be defined has a few mandatory parameters, that help in the definition of the neural net structure. To begin with, each layer is associated with a type (data, conv, relu, pool, etc) and its location - whether it on top of a previous layer, and beneath the next layer. Similarly, the data layer is serves as the input to the&nbsp;ConvNet. </p>
<div class="highlight"><pre>        <span class="n">layer</span> <span class="p">{</span>
          <span class="nl">name:</span> <span class="s">&quot;data&quot;</span>
          <span class="nl">type:</span> <span class="s">&quot;Data&quot;</span>
          <span class="nl">top:</span> <span class="s">&quot;data&quot;</span>
          <span class="nl">top:</span> <span class="s">&quot;label&quot;</span>
          <span class="n">include</span> <span class="p">{</span>
            <span class="nl">phase:</span> <span class="n">TRAIN</span>
          <span class="p">}</span>
          <span class="n">transform_param</span> <span class="p">{</span> 
            <span class="n">mirror</span><span class="o">:</span> <span class="nb">true</span>
            <span class="nl">crop_size:</span> <span class="mi">200</span>
            <span class="nl">mean_file:</span> <span class="s">&quot;mean.binaryproto&quot;</span>
          <span class="p">}</span>
          <span class="n">data_param</span> <span class="p">{</span>
            <span class="nl">source:</span> <span class="s">&quot;path - to - train_lmdb_folder&quot;</span>
            <span class="nl">batch_size:</span> <span class="mi">256</span>
            <span class="nl">backend:</span> <span class="n">LMDB</span>
          <span class="p">}</span>
        <span class="p">}</span>
</pre></div>


<p>As seen, the next layer is a <code>conv</code> layer. Defining layers in Caffe is quite straightforward. Each convolutional layer has a number of required and optional parameters. The required parameters involve num_inputs i.e. input size, and the kernel size. Optional parameters comprise of strides, padding width,&nbsp;etc. </p>
<div class="highlight"><pre>        <span class="n">layer</span> <span class="p">{</span>
            <span class="nl">name:</span> <span class="s">&quot;conv1&quot;</span>
            <span class="nl">type:</span> <span class="s">&quot;Convolution&quot;</span>
            <span class="nl">bottom:</span> <span class="s">&quot;data&quot;</span>
            <span class="nl">top:</span> <span class="s">&quot;conv1&quot;</span>

            <span class="p">....</span>
            <span class="p">....</span>
            <span class="p">....</span>

            <span class="n">convolution_param</span> <span class="p">{</span>
                <span class="nl">num_output:</span> <span class="mi">96</span>
                <span class="nl">kernel_size:</span> <span class="mi">11</span>
                <span class="nl">stride:</span> <span class="mi">4</span>
                <span class="p">...</span>
                <span class="p">...</span>
                <span class="p">...</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></div>


<p>Moving on, we talked about Pool and ReLU layers before as an integral part of ConvNets. Here&#8217;s how to define&nbsp;them.</p>
<div class="highlight"><pre>        <span class="n">layer</span><span class="p">{</span>                              <span class="n">layer</span><span class="p">{</span>
            <span class="nl">name:</span> <span class="s">&quot;relu1&quot;</span>                       <span class="n">name</span><span class="o">:</span> <span class="s">&quot;pool1&quot;</span>
            <span class="nl">type:</span> <span class="s">&quot;ReLU&quot;</span>                        <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Pooling&quot;</span>
            <span class="nl">bottom:</span> <span class="s">&quot;conv1&quot;</span>                     <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;norm1&quot;</span>
            <span class="nl">top:</span> <span class="s">&quot;conv1&quot;</span>                        <span class="n">top</span><span class="o">:</span> <span class="s">&quot;pool1&quot;</span>
        <span class="p">}</span>                                       <span class="n">pooling_param</span><span class="p">{</span>
                                                    <span class="nl">pool:</span> <span class="n">MAX</span> <span class="err">#</span> <span class="n">MAX</span> <span class="n">POOL</span> <span class="n">algorithm</span>
                                                    <span class="n">kernel_size</span> <span class="o">:</span> <span class="mi">3</span>
                                                    <span class="nl">stride:</span> <span class="mi">2</span>
                                                <span class="p">}</span>
</pre></div>


<p><br>
A ReLU will simply activate the convolution layer output and essentially threshold it to 0. (refer Rectifier activation function). Whereas the pooling will reduce the size of the output. AlexNet also uses a normalization layer, which is not much used nowadays.  Similarly, one can keep defining layers in order according to the architecture you want. Lastly, each ConvNet has a fully connected layer, where the input is a column vector ( 1&nbsp;D). </p>
<div class="highlight"><pre>        <span class="n">layer</span> <span class="p">{</span>
            <span class="nl">name:</span> <span class="s">&quot;loss&quot;</span>
            <span class="nl">type:</span> <span class="s">&quot;SoftmaxWithLoss&quot;</span>
            <span class="nl">bottom:</span> <span class="s">&quot;fc8&quot;</span>
            <span class="nl">bottom:</span> <span class="s">&quot;label&quot;</span>
            <span class="nl">top:</span> <span class="s">&quot;loss&quot;</span>
        <span class="p">}</span>
</pre></div>


<p><br></p>
<p>You can view the <a href="https://gist.github.com/kushalvyas/31e595bf1fca3a2dd50227ab524427a7" target="_blank">complete AlexNet architecture :&nbsp;gist</a></p>
<p><strong>Training your data</strong>:</p>
<p>Training can be done in either ways. You can write a python script (which I will update to the blog in a few days)  or you can use the <em>caffe tool</em> to do so. For now, I&#8217;ll be explaining w.r.t the caffe tool which can be found in <code>$caffe_root_dir/build/tools/caffe</code></p>
<p>Once the network architecture has been fixed, all your training lmdb files created the next step is to define the Caffe Solver, defining the learning rate, momentum, solving mode (either <span class="caps">CPU</span> or <span class="caps">GPU</span>), and path for saving snapshots of the&nbsp;model.</p>
<div class="highlight"><pre>        <span class="nl">net:</span> <span class="s">&quot;model.prototxt&quot;</span>
        <span class="nl">base_lr:</span> <span class="mf">0.01</span>
        <span class="nl">lr_policy:</span> <span class="s">&quot;step&quot;</span>
        <span class="nl">gamma:</span> <span class="mf">0.1</span>
        <span class="nl">stepsize:</span> <span class="mi">100000</span>
        <span class="nl">display:</span> <span class="mi">5</span>
        <span class="nl">max_iter:</span> <span class="mi">4500</span>
        <span class="nl">momentum:</span> <span class="mf">0.9</span>
        <span class="nl">weight_decay:</span> <span class="mf">0.0005</span>
        <span class="nl">snapshot:</span> <span class="mi">1000</span>
        <span class="nl">snapshot_prefix:</span> <span class="s">&quot;snapshots1/caltechNET_train&quot;</span>
        <span class="nl">solver_mode:</span> <span class="n">CPU</span>
</pre></div>


<p><br>
Make sure that the solver contains the correct name of the neural net&nbsp;model.</p>
<p><em>Computing Image means</em> : <a href="http://caffe.berkeleyvision.org/gathered/examples/imagenet.html" target="_blank">The AlexNet model requires to subtract each image instance from the mean</a>. You can refer to the <code>caffe_root_dir/examples/imagenet/make_imagenet_mean.sh</code> file to compute the&nbsp;mean.</p>
<p>The  model, solver, means file and lmdb imagesets have been made. Next, is to sit and train the model. This can be done using the caffe executable&nbsp;file.</p>
<p><code>$caffe_root_dir/build/tools/caffe train --solver=solver.prototxt</code></p>
<p>Literally, sit back and enjoy ! Your snapshots directory will get updated with the model snapshots as and&nbsp;when.</p>
<p><br></p>
<p><strong>Testing</strong>:</p>
<p>The final trained net is available in 
<code>snapshots/&lt;file_iter_number&gt;.caffemodel</code> file. There are a few tricks to test a particular&nbsp;image.</p>
<p>Firstly, it is important that we map the training classes to their respective class names. If you see above, in the part where the data was being prepared, we made a txtfile of the following&nbsp;pattern</p>
<p>&lt;path_to_caltech101>/limited/train/sunflower/image_0020.jpg&nbsp;0</p>
<p>The class &#8216;sunflower&#8217; has been mapped to 0. and so on with other classes.
<br></p>
<table>
<thead>
<tr>
<th>Class name</th>
<th>Mapped to class number</th>
</tr>
</thead>
<tbody>
<tr>
<td>sunflower</td>
<td>0</td>
</tr>
<tr>
<td>Motorbike</td>
<td>1</td>
</tr>
<tr>
<td>dollar_bill</td>
<td>2</td>
</tr>
<tr>
<td>airplanes</td>
<td>3</td>
</tr>
<tr>
<td>soccer_ball</td>
<td>4</td>
</tr>
<tr>
<td>Faces</td>
<td>5</td>
</tr>
<tr>
<td>accordion</td>
<td>6</td>
</tr>
</tbody>
</table>
<p><br>
Next, the caffe model needs to be&nbsp;loaded. </p>
<p><br></p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">Classifier</span><span class="p">(</span><span class="s">&quot;path_to_model.prototxt&quot;</span><span class="p">,</span> <span class="s">&quot;snapshot_file.caffemodel&quot;</span><span class="p">,</span> 
                       <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;means.npy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">channel_swap</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
                       <span class="n">raw_scale</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                       <span class="n">image_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">im</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="s">&quot;path_to_image&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">im</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">class_id</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="c"># this returns the class id or the class number</span>

<span class="o">...</span> 
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="n">classname</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span> <span class="c"># to print the name of the class</span>
</pre></div>


<p><br><br>
<strong>Some tricks</strong>&nbsp;: </p>
<p>There may be some cases where the means file is stored as a means.binaryproto. There is a quick way to convert it to a npy file. <a href="https://github.com/BVLC/caffe/issues/808" target="_blank">Refer issue #808</a> for the conversion of .binaryproto to .npy files.
<br></p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="c"># solution github issue 808 by @mafiosso</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">channels</span> <span class="o">=</span> <span class="mi">3</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">blob</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">caffe_pb2</span><span class="o">.</span><span class="n">BlobProto</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;mean.binaryproto&#39;</span><span class="p">,</span> <span class="s">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">blob</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">means</span> <span class="o">=</span> <span class="n">blob</span><span class="o">.</span><span class="n">data</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="n">means</span><span class="o">.</span><span class="n">shape</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="c"># size of image. </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">means</span> <span class="o">=</span> <span class="n">means</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&quot;means.npy&quot;</span><span class="p">,</span> <span class="n">means</span><span class="p">)</span>
</pre></div>


<p><br>
It is highly recommended that you use <span class="caps">GPU</span>&#8217;s to tarin networks. However, feel free to experiment with your personal computers. Works just fine&nbsp;!</p>
<h2>Classification&nbsp;results</h2>
<p>The Caltech101 dataset limited directory already has a split of data into train and&nbsp;test. </p>
<p><center></p>
<p>There are a few snaps of outputs when the model was tested on the limited version of the dataset&nbsp;.</p>
<p><img alt="im1" src="http://kushalvyas.github.io/images/cnn_images/new_a.png" />
<img alt="im2" src="http://kushalvyas.github.io/images/cnn_images/new_airplane.png" />
<img alt="im3" src="http://kushalvyas.github.io/images/cnn_images/new_soccer_ball.png" /><br>
<img alt="im4" src="http://kushalvyas.github.io/images/cnn_images/new_motorbike.png" />
<img alt="im5" src="http://kushalvyas.github.io/images/cnn_images/new_sunflower.png" /></p>
<p></center></p>
<p>So, we have completed the tutorial on how to create a custom dataset and train it using caffe. Now you can implement this and train any dataset you want. I would recommend reading up on the references to get a better understanding of&nbsp;ConvNets.</p>
<p><br>
<strong>I will keep updating this article with newly pretrained models and adding more about python interfacing with Caffe. Till then, have fun implementing <span class="caps">CNN</span>&#8217;s.</strong></p>
<p><br><br><br></p>
<h2>References:</h2>
<p>[1] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank">Convolutional Neural Networks for Visual Recognition - CS231n&nbsp;Stanford</a></p>
<p>[2] <a href="http://caffe.berkeleyvision.org" target="_blank">Berkely Vision Lab -&nbsp;Caffe</a></p>
<p>[3] <a href="http://caffe.berkeleyvision.org/gathered/examples/imagenet.html" target="_blank">ImageNet tutorial - Caffe&nbsp;Docs</a></p>
<p>[4] <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank">AlexNet&nbsp;Paper</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>
    </section>
    <footer class="footer-article">
        <div class="tags-and-categories">Filed under <a href="http://kushalvyas.github.io/category/object-recognition-classification-cv.html">Object Recognition, Classification, CV</a>
        | Tagged: <a href="http://kushalvyas.github.io/tag/objectrecognition.html">ObjectRecognition </a>
        | <a href="http://kushalvyas.github.io/caffe_cnn.html" rel="bookmark"
         title="Permalink to Caffe + ConvNets : Visual Recognition MadeÂ Easy">Permalink</a>            
        </div>
        <!-- metaldata 
        -->
    </footer>
    <!-- disqus bitsmakemecrazy -->
        <hr>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = "bitsmakemecrazy"; // required: replace example with your forum shortname
            var disqus_identifier = "caffe-+-convnets-:-visual-recognition-made-easy";
            var disqus_url = "http://kushalvyas.github.io/caffe_cnn.html";
            var disqus_title = "Caffe + ConvNets : Visual Recognition Made Easy";
            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
            </div>
         
            <div class="large-2 aside column" id="SIDEID">
         
                <!-- <h3>Trending</h3>
                <ul>

                    <li><a href="http://kushalvyas.github.io/caffe_cnn.html">Caffe + ConvNets</a></li>

                    <li><a href="http://kushalvyas.github.io/stitching.html">Image Stitching -  A Detailed Tutorial</a></li>

                    <li><a href="http://kushalvyas.github.io/BOV.html#BOV">Bag of Visual Words</a></li>

                    <li><a href="http://kushalvyas.github.io/wiki_lookup.html">Wiki Lookup</a></li>
                </ul>
                <h3>Coming Soon</h3>
                <ul>
                        <li><a href="#" onclick="executeComingSoonPopUp();">Point clouds</a></li>
                        <li><a href="#" onclick="executeComingSoonPopUp();">LIDARs</a></li>
                </ul> -->
            </div>
            <script>
                function load_sideID(){
                    var curr_url=  document.URL;
                    if(curr_url.indexOf('projects')>0){
                        console.log("on project page");
                    }else{
                        console.log("not on project page");
                        var side= $("#SIDEID");
                        var data = "<h3>Trending</h3>";
                        data += "<ul>";
                           data += '<li><a href="http://kushalvyas.github.io/caffe_cnn.html">Caffe + ConvNets</a></li>';
                           data += '<li><a href="http://kushalvyas.github.io/stitching.html">Image Stitching -  A Detailed Tutorial</a></li>';
                           data += '<li><a href="http://kushalvyas.github.io/BOV.html#BOV">Bag of Visual Words</a></li>';
                           data += '<li><a href="http://kushalvyas.github.io/wiki_lookup.html">Wiki Lookup</a></li>';
                        data += "</ul>";

                        data +="<h3>Coming Soon</h3>";
                        data+="<ul>";
                            data+='<li><a href="">Point clouds</a></li>';
                            data+='<li><a href="">LIDARs</a></li>';
                        data += '</ul>';
                        side.append(data);
                    }
                }
                load_sideID();

            </script>
          
         </div>
    </div>
</div>
    <div class="footer">
        <div class="data-holder">
        <div class="row">
            <div class="large-3 column">
<div class="credits">
    <h3>Stuff and credits</h3>
    <p>
    Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, 
    which takes great advantage of <a href="http://python.org" target="_blank">Python</a>.
    </p>
    <p>Theme is a modified Pelican Bricks</p>
    <p>
    This site also makes use of <a href="http://foundation.zurb.com" target="_blank">Zurb Foundation Framework</a>
    and is typeset using the blocky -- but quite good-looking indeed -- 
    <a href="https://www.google.com/fonts/specimen/Exo%202" target="_blank">Exo 2</a> fonts,
    which comes in a lot of weight and styles. 
    </p>
    <p>Enjoy!</a>
</div>           </div>
           <div class="large-6 tag-cloud column">
                <h3>Tags</h3>
                <ul>
                    <li><a href="http://kushalvyas.github.io/tag/stitching.html">stitching</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/mosaicking.html">mosaicking</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/ai.html">AI</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/utils.html">utils</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/py.html">Py</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/wikipedia.html">wikipedia</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/dev.html">dev</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/technical.html">technical</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/images.html">images</a>(1)</li>
                    <li><a href="http://kushalvyas.github.io/tag/cv.html">CV</a>(4)</li>
                    <li><a href="http://kushalvyas.github.io/tag/objectrecognition.html">ObjectRecognition</a>(3)</li>
                </ul>
            </div>
            <div class="large-3 category-column column">
                <h3>Categories</h3>
                <ul>
                    <li><a href="http://kushalvyas.github.io/category/ai.html">AI</a> (1)</li>
                    <li><a href="http://kushalvyas.github.io/category/cv.html">CV</a> (2)</li>
                    <li><a href="http://kushalvyas.github.io/category/cv-ip.html">CV, IP</a> (1)</li>
                    <li><a href="http://kushalvyas.github.io/category/graph_py.html">graph_py</a> (1)</li>
                    <li><a href="http://kushalvyas.github.io/category/networks.html">networks</a> (1)</li>
                    <li class="active"><a href="http://kushalvyas.github.io/category/object-recognition-classification-cv.html">Object Recognition, Classification, CV</a> (2)</li>
                    <li><a href="http://kushalvyas.github.io/category/object-recognitioncv.html">Object Recognition,CV</a> (1)</li>
                    <li><a href="http://kushalvyas.github.io/category/technical-dev.html">technical, dev</a> (1)</li>
                    <li><a href="http://kushalvyas.github.io/category/utils.html">utils</a> (1)</li>
                </ul>
             
            </div>
        </row>
        </div>
    </div>

</body>
</html>